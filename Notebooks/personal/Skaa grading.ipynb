{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tools for grading portions of a super kick ass assignment__\n",
    "\n",
    "created: 23 April 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Dynamically sets location so do not have to manually toggle between dropbox and documents\n",
    "import os\n",
    "path = \"/\".join([a for a in os.path.abspath(\"\").split('/') if a not in ['Notebooks', 'personal']])\n",
    "%cd $path\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "from CanvasHacks import environment\n",
    "from CanvasHacks.Configuration import InteractiveConfiguration\n",
    "# from CanvasHacks.PeerReviewed.Definitions import TopicalAssignment, ContentAssignment\n",
    "from CanvasHacks.Models.QuizModels import QuizData\n",
    "\n",
    "# Requests\n",
    "from CanvasHacks.Api.RequestTools import get_all_course_assignments, get_assignments_needing_grading\n",
    "from CanvasHacks.Api.RequestTools import send_get_request, send_post_request, send_put_request, make_url, make_request_header\n",
    "from CanvasHacks.Api.RequestTools import make_request_header, get_all_course_quizzes, get_all_quiz_submissions\n",
    "import requests\n",
    "\n",
    "from CanvasHacks.Api.UrlTools import make_url\n",
    "from CanvasHacks.Api.UploadGradeTools import make_upload_button\n",
    "import CanvasHacks.Api.DownloadProcessingTools as PT\n",
    "\n",
    "# Repos\n",
    "from CanvasHacks.Repositories.DataManagement import DataStore\n",
    "from CanvasHacks.Repositories.factories import WorkRepositoryFactory, WorkRepositoryLoaderFactory\n",
    "from CanvasHacks.Repositories.quizzes import QuizRepository, ReviewRepository\n",
    "from CanvasHacks.Repositories.codes import AccessCodeRepo\n",
    "from CanvasHacks.Repositories.reviewer_associations import assign_reviewers, AssociationRepository\n",
    "from CanvasHacks.Repositories.students import StudentRepository\n",
    "from CanvasHacks.Repositories.submissions import SubmissionRepository, QuizSubmissionRepository\n",
    "# from CanvasHacks.Repositories.quizzes import process_work, load_student_work, remove_non_final_attempts, make_drop_list, save_json, drop_columns_from_frame\n",
    "\n",
    "\n",
    "# Filesystem\n",
    "from CanvasHacks.Files.FileTools import makeDataFileIterator, create_folder\n",
    "from CanvasHacks.Files.QuizReportFileTools import load_new\n",
    "# from CanvasHacks.QuizReportFileTools import  make_quiz_repo, load_activity_data_from_files\n",
    "\n",
    "# Widgets\n",
    "from CanvasHacks.Widgets.AssignmentSelection import make_assignment_chooser, view_selected_assignments, view_ungraded_assignments\n",
    "from CanvasHacks.Widgets.LiveSelection import make_test_selector\n",
    "from CanvasHacks.Widgets.AssignmentSelection import make_unit_chooser, make_selection_button\n",
    "from CanvasHacks.Widgets.ConsolidatedTextOutput import make_assignment_header, make_consolidated_text_fields\n",
    "from CanvasHacks.Widgets.InputFields import make_course_ids_input, make_canvas_token_input, make_canvas_url_input, make_general_reset_button\n",
    "\n",
    "\n",
    "#Grading\n",
    "from CanvasHacks.GradingHandlers.quiz import QuizGrader\n",
    "# import CanvasHacks.GradingTools.nonempty as GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!\n",
    "# This must be manually adjusted if going to give \n",
    "# quarter credit\n",
    "\n",
    "# Use this date for things due before the midterm\n",
    "# QUARTER_CREDIT_DATE =  '2019-10-16T07:59:00Z'\n",
    "\n",
    "# Date for things due after the midterm\n",
    "# QUARTER_CREDIT_DATE =  '2020-12-18T07:59:00Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo\n",
    "\n",
    "    make_assignment_chooser is not picking up all assignments needing grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# assigns = get_all_course_assignments( environment.CONFIG.course_ids[0] )\n",
    "# len(assigns)\n",
    "get_assignments_needing_grading(environment.CONFIG.course_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev hotfix workaround for F25\n",
    "environment.CONFIG.set_unit(1)\n",
    "\n",
    "\n",
    "assignments=[]\n",
    "for course_id in environment.CONFIG.course_ids:\n",
    "    assignments += get_assignments_needing_grading( course_id)\n",
    "\n",
    "    # assignments += get_assignments_with_submissions( course_id, **kwargs )\n",
    "assignments = [ (a[ 'id' ], a[ 'name' ]) for a in assignments ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get quiz ids and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "make_test_selector()\n",
    "make_unit_chooser(num_units=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "make_assignment_chooser(needs_grading=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.GradingMethods.nonempty import CreditForNonEmptyPoints\n",
    "from CanvasHacks.GradingMethods.review import ReviewBasedPoints\n",
    "from CanvasHacks.GradingMethods.wordcount import GradeByWordCountPoints\n",
    "\n",
    "# unit 1\n",
    "# REVIEW_COLUMNS = [\"3868950: Having read the author's essay carefully, I feel it deserves this many points.\"]\n",
    "\n",
    "# todo make a selector which let's choose without messing up formatting\n",
    "# unit 2\n",
    "# REVIEW_COLUMNS = [\"\"\"4082297: Having read the author's essay carefully, I feel it deserves this many points.\n",
    "# [Remember, the essay grade is: 1 point for completion; 3 from the reviewer (what you're assigning here); and 3 from the instructor]\"\"\"]\n",
    "\n",
    "# REVIEW_COLUMNS = [\"\"\"4082297: \\nHaving read the author's essay carefully, I feel it deserves this many points.\\n[Remember, the essay grade is: 1 point for completion; 3 from the reviewer (what you're assigning here); and 3 from the instructor]\\n\"\"\"]\n",
    "\n",
    "# This holds the selected column in which the reviewer assigns points\n",
    "REVIEW_COLUMNS = []\n",
    "\n",
    "# word count grading thresholds\n",
    "thresholds = [\n",
    "    {\n",
    "        'min_count': 600,\n",
    "        'points': 6\n",
    "    },\n",
    "     {\n",
    "        'min_count': 500,\n",
    "        'points': 5\n",
    "    },\n",
    "     {\n",
    "        'min_count': 400,\n",
    "        'points': 4\n",
    "    },\n",
    "    {\n",
    "        'min_count': 300,\n",
    "        'points': 3\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'min_count': 100,\n",
    "        'points': 1\n",
    "    },\n",
    "]\n",
    "\n",
    "cnep = CreditForNonEmptyPoints(max_possible_points=2)\n",
    "\n",
    "# rbp = ReviewBasedPoints(\n",
    "#     graded_activity=environment.CONFIG.unit.initial_work,\n",
    "#     review_activity=environment.CONFIG.unit.review,\n",
    "#     review_columns=REVIEW_COLUMNS,\n",
    "#     max_possible_points=3)\n",
    "\n",
    "wcp = GradeByWordCountPoints(max_possible_points=6, threshold_dicts=thresholds)\n",
    "\n",
    "environment.CONFIG.unit.initial_work.grade_methods = [cnep,  wcp]\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets, VBox\n",
    "\n",
    "def make_review_column_chooser(review_columns):\n",
    "    def adder(item_id, item_name):\n",
    "        review_columns.append(item_name)\n",
    "\n",
    "    def getter():\n",
    "        return review_columns \n",
    "\n",
    "    def remover(item_name):\n",
    "        el = list( filter( lambda x: x[ 0 ] == item_name, review_columns ) )[ 0 ]\n",
    "        idx = review_columns.index( el )\n",
    "        return review_columns.pop( idx )\n",
    "\n",
    "    cols = [c for c in rbp.review_repo.data.columns if len(c) >10]\n",
    "    buttons = [make_selection_button(item_id=c, name=c, get_func=getter, add_func=adder, width='100%', remove_func=remover ) for c in cols]\n",
    "    head = widgets.HTML( value=\"<h4>Select the question in which reviewers determine score</h4>\" )             \n",
    "\n",
    "    display( VBox( [VBox([head]), VBox(buttons)] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev CAN-72 Assign preliminary essay grade based on review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# F20\n",
    "\n",
    "from CanvasHacks.GradingMethods.nonempty import CreditForNonEmptyPoints\n",
    "from CanvasHacks.GradingMethods.review import ReviewBasedPoints\n",
    "from CanvasHacks.GradingMethods.wordcount import GradeByWordCountPoints\n",
    "\n",
    "# unit 1\n",
    "# REVIEW_COLUMNS = [\"3868950: Having read the author's essay carefully, I feel it deserves this many points.\"]\n",
    "\n",
    "# todo make a selector which let's choose without messing up formatting\n",
    "# unit 2\n",
    "# REVIEW_COLUMNS = [\"\"\"4082297: Having read the author's essay carefully, I feel it deserves this many points.\n",
    "# [Remember, the essay grade is: 1 point for completion; 3 from the reviewer (what you're assigning here); and 3 from the instructor]\"\"\"]\n",
    "\n",
    "# REVIEW_COLUMNS = [\"\"\"4082297: \\nHaving read the author's essay carefully, I feel it deserves this many points.\\n[Remember, the essay grade is: 1 point for completion; 3 from the reviewer (what you're assigning here); and 3 from the instructor]\\n\"\"\"]\n",
    "\n",
    "# This holds the selected column in which the reviewer assigns points\n",
    "REVIEW_COLUMNS = []\n",
    "\n",
    "# word count grading thresholds\n",
    "thresholds = [\n",
    "    {\n",
    "        'min_count': 600,\n",
    "        'points': 3\n",
    "    },\n",
    "    {\n",
    "        'min_count': 300,\n",
    "        'points': 2\n",
    "    },\n",
    "    {\n",
    "        'min_count': 100,\n",
    "        'points': 1\n",
    "    },\n",
    "]\n",
    "\n",
    "cnep = CreditForNonEmptyPoints(max_possible_points=1)\n",
    "\n",
    "rbp = ReviewBasedPoints(\n",
    "    graded_activity=environment.CONFIG.unit.initial_work,\n",
    "    review_activity=environment.CONFIG.unit.review,\n",
    "    review_columns=REVIEW_COLUMNS,\n",
    "    max_possible_points=3)\n",
    "\n",
    "wcp = GradeByWordCountPoints(max_possible_points=3, threshold_dicts=thresholds)\n",
    "\n",
    "environment.CONFIG.unit.initial_work.grade_methods = [cnep, rbp, wcp]\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets, VBox\n",
    "\n",
    "def make_review_column_chooser(review_columns):\n",
    "    def adder(item_id, item_name):\n",
    "        review_columns.append(item_name)\n",
    "\n",
    "    def getter():\n",
    "        return review_columns \n",
    "\n",
    "    def remover(item_name):\n",
    "        el = list( filter( lambda x: x[ 0 ] == item_name, review_columns ) )[ 0 ]\n",
    "        idx = review_columns.index( el )\n",
    "        return review_columns.pop( idx )\n",
    "\n",
    "    cols = [c for c in rbp.review_repo.data.columns if len(c) >10]\n",
    "    buttons = [make_selection_button(item_id=c, name=c, get_func=getter, add_func=adder, width='100%', remove_func=remover ) for c in cols]\n",
    "    head = widgets.HTML( value=\"<h4>Select the question in which reviewers determine score</h4>\" )             \n",
    "\n",
    "    display( VBox( [VBox([head]), VBox(buttons)] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "make_review_column_chooser(REVIEW_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.executables.GradeAssignmentStep import GradeAssignment\n",
    "\n",
    "WAIT =0\n",
    "\n",
    "NO_LATE_PENALTY =True\n",
    "\n",
    "UPLOAD_GRADES = True\n",
    "# UPLOAD_GRADES = False\n",
    "\n",
    "# hack because selector isn't picking up the essays\n",
    "step = GradeAssignment(activity=environment.CONFIG.unit.initial_work, \n",
    "                       rest_timeout=WAIT, \n",
    "                       no_late_penalty=NO_LATE_PENALTY, \n",
    "                       upload_grades=UPLOAD_GRADES )\n",
    "# step = GradeAssignment(rest_timeout=WAIT, no_late_penalty=NO_LATE_PENALTY, upload_grades=UPLOAD_GRADES )\n",
    "\n",
    "step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step.workRepo.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step.grade_records[1].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade (post-CAN-44)\n",
    "## Grade review (quiz type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAN-72 dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cnep = CreditForNonEmptyPoints(max_possible_points=1)\n",
    "\n",
    "rbp = ReviewBasedPoints(\n",
    "    graded_activity=environment.CONFIG.unit.initial_work,\n",
    "    review_activity=environment.CONFIG.unit.review,\n",
    "    review_columns=REVIEW_COLUMNS,\n",
    "    max_possible_points=2)\n",
    "\n",
    "environment.CONFIG.unit.review.grade_methods = [cnep]\n",
    "environment.CONFIG.unit.review.corrections = [rbp]\n",
    "\n",
    "make_review_column_chooser(REVIEW_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may have uploaded scores of 1 instead of 0!!!! for Unit 3 warm up\n",
    "\n",
    "\n",
    "from CanvasHacks.executables.GradeQuizStep import GradeQuiz\n",
    "\n",
    "WAIT =5\n",
    "NO_LATE_PENALTY =True\n",
    "UPLOAD_GRADES = True\n",
    "\n",
    "step = GradeQuiz(rest_timeout=WAIT, \n",
    "                 no_late_penalty=NO_LATE_PENALTY, \n",
    "                 upload_grades=UPLOAD_GRADES )\n",
    "#step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre CAN-72 grade quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may have uploaded scores of 1 instead of 0!!!! for Unit 3 warm up\n",
    "\n",
    "\n",
    "from CanvasHacks.executables.GradeQuizStep import GradeQuiz\n",
    "\n",
    "WAIT =5\n",
    "NO_LATE_PENALTY =True\n",
    "UPLOAD_GRADES = True\n",
    "\n",
    "step = GradeQuiz(rest_timeout=WAIT, \n",
    "                 no_late_penalty=NO_LATE_PENALTY, \n",
    "                 upload_grades=UPLOAD_GRADES )\n",
    "#step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step.graded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade assignment type activities\n",
    "\n",
    "Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.executables.GradeAssignmentStep import GradeAssignment\n",
    "\n",
    "WAIT =0\n",
    "\n",
    "NO_LATE_PENALTY =True\n",
    "UPLOAD_GRADES = False\n",
    "\n",
    "# hack because selector isn't picking up the essays\n",
    "step = GradeAssignment(activity=environment.CONFIG.unit.initial_work, \n",
    "                       rest_timeout=WAIT, \n",
    "                       no_late_penalty=NO_LATE_PENALTY, \n",
    "                       upload_grades=UPLOAD_GRADES )\n",
    "# step = GradeAssignment(rest_timeout=WAIT, no_late_penalty=NO_LATE_PENALTY, upload_grades=UPLOAD_GRADES )\n",
    "\n",
    "step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step.graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- Dev ------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grade on wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.GradingMethods.wordcount import GradeByWordCount, make_pct_required_count_thresholds\n",
    "\n",
    "\n",
    "REQUIRED_CNT = 1000\n",
    "thresholds = make_pct_required_count_thresholds(REQUIRED_CNT)\n",
    "grader = GradeByWordCount(thresholds)\n",
    "\n",
    "ddd = step1.work_repo.data.copy(deep=True)\n",
    "\n",
    "ddd['grade'] = ddd.apply(lambda x: grader.grade(x.body), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather statistics for assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and store wordbags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = [c for c in environment.CONFIG.unit.components if c.id == environment.CONFIG.assignments[0][0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the characteristics of the quizzes that we are going to grade\n",
    "qs = []\n",
    "for s in environment.CONFIG.course_ids:\n",
    "    qs += get_all_course_quizzes(s)\n",
    "to_grade = [assign_id for assign_id, name in environment.CONFIG.assignments]\n",
    "quizzes = [QuizData(**q) for q in qs if q['assignment_id'] in to_grade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically get data\n",
    "workRepo = make_quiz_repo(environment.CONFIG.course, activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workRepo = QuizRepository(, environment.CONFIG.course)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workRepo.points_per_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.CONFIG.unit.topical_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make folders to store quiz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(environment.CONFIG.unit.topical_assignment.folder_path)\n",
    "# for q in quizzes:\n",
    "#     create_folder(q.folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on CAN-41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_activity_data_from_files(activity, course):\n",
    "    \"\"\"Get complete set of data for activity from a bunch of \n",
    "    potentially inconsistent files. \n",
    "    Created in CAN-41\n",
    "    Loads the data into a repository object of the appropriate type\n",
    "    \"\"\"\n",
    "    fiter = makeDataFileIterator(activity.folder_path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            f = next(fiter)\n",
    "            print(\"loading: \", f)\n",
    "            f = pd.read_csv(f)\n",
    "            if 'student_id' not in f.columns:\n",
    "                f.rename( { 'id': 'student_id' }, axis=1, inplace=True )\n",
    "            frames.append(f)\n",
    "    except StopIteration:\n",
    "        print(\"Loaded data from {} files\".format(len(frames)))\n",
    "\n",
    "        data = pd.concat(frames, sort=True)\n",
    "        # drop_columns_from_frame(f)\n",
    "        # f.drop(EXTRANEOUS_COLUMNS, axis=1, inplace=True)\n",
    "        # repo = ReviewRepository(activity)\n",
    "\n",
    "        print(\"Loaded {} rows from all files in folder\".format(len(f)))\n",
    "\n",
    "        # this shouldn't be necessary in future. Only when were saving sheet plus submissions\n",
    "        if 'score_x' in data.columns:\n",
    "            def fix_score(row):\n",
    "                return row[['score', 'score_x', 'score_y']].mean()\n",
    "            data.score = data.apply(lambda x: fix_score(x), axis=1)\n",
    "            data.drop([ 'score_x', 'score_y'], axis=1, inplace=True)\n",
    "\n",
    "        repo = WorkRepositoryFactory.make(activity, course)\n",
    "        repo.set_question_columns(data)\n",
    "        drop_columns_from_frame(data)\n",
    "\n",
    "        # f = f[ repo.keep_columns ]\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        repo.data = data\n",
    "        print(\"{} rows loaded to repo.data after processing\".format(len(repo.data)))\n",
    "        return repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workRepo = load_activity_data_from_files(activity, environment.CONFIG.course)\n",
    "\n",
    "# This is necessary if using post CAN-44\n",
    "data = load_activity_data_from_files(activity, environment.CONFIG.course)\n",
    "workRepo = WorkRepositoryFactory.make( activity, environment.CONFIG.course )\n",
    "workRepo.data = data\n",
    "workRepo.set_question_columns( data )\n",
    "# print( \"{} rows loaded to repo.data after processing\".format( len( workRepo.data ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workRepo.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRANEOUS_COLUMNS = ['has_seen_results',\n",
    "       'html_url',  'overdue_and_needs_submission',\n",
    "       'quiz_points_possible','validation_token',\n",
    "       'section_id', '_requester', 'attempts_left',\n",
    "       'end_at', 'end_at_date', 'excused?', 'extra_attempts', 'extra_time',\n",
    "       'finished_at', \n",
    "       'started_at', 'attributes', 'course_id', \n",
    " 'manually_unlocked', 'n correct',\n",
    "       'n incorrect','quiz_version', 'result_url',\n",
    "       'score_before_regrade', 'section', 'sis_id',\n",
    "       'started_at_date', 'submitted',  'kept_score'\n",
    "       'time_spent',  'workflow_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_COLUMNS = ['attempt', 'course_id', 'finished_at_date',\n",
    "       'fudge_points', 'id', 'name', 'quiz_id', 'score',\n",
    "       'section_sis_id', 'student_id', 'submission_id', 'user_id',  'workflow_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ a for a in f.attempt if a >1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "g = f.groupby('student_id')\n",
    "j = f.set_index(['student_id', 'attempt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "for n, group in g:\n",
    "#     r.append( j.loc[n, group.attempt.max()])\n",
    "    r.append((n, group.attempt.max()))\n",
    "# r = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(j.loc[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.loc[r].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(group):\n",
    "    latest_attempt = group.attempt.max()\n",
    "    print(latest_attempt)\n",
    "#     return \n",
    "    \n",
    "g.filter(lambda x: x.attempt == x.attempt.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[['student_id', 'user_id', 'attempt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[:3].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download submission objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_all_quiz_submissions(activity.course_id, activity.quiz_id)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Get all submissions and save them in a dataframe\n",
    "# We will need the ids when it comes time to upload grades (I think)\n",
    "\n",
    "submissions = []\n",
    "\n",
    "for quiz in quizzes:\n",
    "    results = get_all_quiz_submissions(quiz.course_id, quiz.id)\n",
    "    for r in results:\n",
    "        submissions.append({'course_id' : int(quiz.course_id), \n",
    "                             'quiz_id' : int(quiz.id),\n",
    "                             'student_id' : int(r['user_id']), \n",
    "                             'submission_id': int(r['id']),\n",
    "                             'attempt' : int(r['attempt']) \n",
    "                            })\n",
    "submissions = pd.DataFrame(submissions)\n",
    "len(submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API doesn't allow to download individual student answers. It does allow to download the answers in a csv via the student analysis report.\n",
    "\n",
    "Log into Canvas, go to the assignment --> Quiz Statistics --> Student analysis\n",
    "\n",
    "See https://github.com/instructure/canvas-lms/issues/742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load student answers from csv files\n",
    "\n",
    "Before doing this, the Student Analysis reports need to be manually downloaded from canvas and placed in the correct folder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade based on emptiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the current version. Should be able to handle\n",
    "# first-run-grading or any subsequent run\n",
    "\n",
    "NO_LATE_PENALTY = True\n",
    "\n",
    "graded = []\n",
    "selected_ids = [a[0] for a in environment.CONFIG.assignments]\n",
    "to_grade = [a for a in environment.CONFIG.unit.components if a.id in selected_ids]\n",
    "\n",
    "# to_grade = [environment.CONFIG.unit.initial_work]\n",
    "for activity in to_grade:\n",
    "    # If don't just want to use lock date for determining \n",
    "    # last chance for half credit (e.g., midterm for quarter credit), do this\n",
    "    # activity.quarter_credit_deadline = QUARTER_CREDIT_DATE\n",
    "    if NO_LATE_PENALTY:\n",
    "        activity.due_at = activity.lock_at\n",
    "    # Get all submission records for the activity\n",
    "    quiz = environment.CONFIG.course.get_quiz(activity.quiz_id)\n",
    "    subRepo = QuizSubmissionRepository(quiz)\n",
    "#     assignment = environment.CONFIG.course.get_assignment(int(activity.id))\n",
    "    \n",
    "    # Load all new records since last download\n",
    "    newframe = load_new(activity)\n",
    "    len(newframe)\n",
    "    student_work = process_work(newframe, subRepo.frame)\n",
    "    remove_non_final_attempts(student_work)\n",
    "    \n",
    "    # Load processed work into a quiz repository\n",
    "    workRepo = QuizRepository(activity, environment.CONFIG.course)\n",
    "    workRepo.set_question_columns(student_work)\n",
    "    workRepo.data = student_work\n",
    "    \n",
    "    # Let the grader do its job\n",
    "    grader = QuizGrader(workRepo, subRepo)\n",
    "    g = grader.grade()\n",
    "    graded += g\n",
    "len(graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workRepo.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subRepo.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET /api/v1/courses/:course_id/quizzes/:quiz_id/submissions/:id \n",
    " \n",
    "PUT /api/v1/courses/:course_id/quizzes/:quiz_id/submissions/:id \n",
    "                \n",
    "{\n",
    "  \"quiz_submissions\": [{\n",
    "    \"attempt\": 1,\n",
    "    \"fudge_points\": -2.4\n",
    "  }]\n",
    "}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# If this is a quiz type assignment, we need to get the quiz submission object\n",
    "# not the regular submission object\n",
    "\n",
    "quiz = environment.CONFIG.course.get_quiz(activity.quiz_id)\n",
    "qsubs = [s for s in quiz.get_submissions()]\n",
    "\n",
    "def get_submission_object(student_id, attempt):    \n",
    "    return [d for d in qsubs if d.user_id == student_id and d.attempt == attempt]\n",
    "\n",
    "# Upload grades\n",
    "for g in graded:\n",
    "    sub = get_submission_object(g['student_id'], g['attempt'])[0]\n",
    "    sub.update_score_and_comments(quiz_submissions=g['data']['quiz_submissions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def upload_quiz_grades( activity, grade_data ):\n",
    "    cnt = 0\n",
    "    for r in grade_data:\n",
    "        url = make_url( activity.course_id, 'quizzes' )\n",
    "#         url = make_url( int(r[ 'course_id' ]), 'assignments' )\n",
    "        url += \"/%s/submissions/%s\" % (activity.quiz_id, r[ 'submission_id' ])\n",
    "#         url += \"/%s/submissions/%s\" % (r[ 'quiz_id' ], r[ 'student_id' ]) #r[ 'submission_id' ])\n",
    "        print(url)\n",
    "        response = send_put_request( url, r[ 'data' ] )\n",
    "        print(response)\n",
    "        cnt += 1\n",
    "    print( \"{} grades uploaded\".format( cnt ) )\n",
    "\n",
    "# Handle the uploading of grades after a \n",
    "# visual sanity check\n",
    "upload_quiz_grades(activity, graded)\n",
    "# for g in graded:\n",
    "#     upload_quiz_grades(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect copied answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "## Combine data from all classes\n",
    "\n",
    "Each class's data will have different names for the quiz questions. \n",
    "Thus before we combine the data frames we need to rename the columns, so it will line up properly\n",
    "\n",
    "- Todo: Once have a better sense of the variations, rewrite using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_q_names(txt):\n",
    "    # remove newlines which make the titles different\n",
    "    txt = txt.replace('\\n', \" \")\n",
    "    # The question title will have the format 000000: text text text text\n",
    "    # so we keep the main body of the text (we will lose anything after a ':' in the text)\n",
    "    txt = txt.split(\":\")[1]\n",
    "    return txt.strip()\n",
    "    \n",
    "\n",
    "for f in frames:\n",
    "    cols = detect_question_columns(f)\n",
    "    # Make a dictionary mapping the old names to the new names\n",
    "    rename_cols = {c: normalize_q_names(c) for c in cols}\n",
    "    f.rename(rename_cols, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "d = pd.concat(frames)\n",
    "\n",
    "drop_columns_from_frame(d)\n",
    "\n",
    "d.columns\n",
    "\n",
    "d.to_excel(\"%s/oa1.xlsx\" %(environment.LOG_FOLDER))\n",
    "\n",
    "\n",
    "data = {}\n",
    "for r in journal_folders:\n",
    "    with open(\"%s/all-submissions.json\" % r, 'r') as f:\n",
    "        j = json.load(f)\n",
    "    data[r.split('/')[-1:][0]] = [make_wordbag(str(row['body'])) for row in j]\n",
    "\n",
    "# data\n",
    "\n",
    "field_name= \"What is an example of persuasive advertising? (Don't use Crisp's examples)\"\n",
    "\n",
    "def make_bag_from_oa_field(frame, field_name):\n",
    "    bag = []\n",
    "    bags = [ make_wordbag(str(row)) for i, row in frame[field_name].iteritems()]\n",
    "    for b in bags:\n",
    "        bag = bag + b\n",
    "    return bag\n",
    "\n",
    "\n",
    "\n",
    "draw_cloud(bag, field_name)\n",
    "\n",
    "fields = [\"What is an example of informational advertising? (Don't use Crisp's examples)\",\n",
    " 'Why does it matter that the desires activated by persuasive advertising are unconscious?',\n",
    " 'Give an example of informational advertising which Crisp might find objectionable',\n",
    " \"Suppose that a movie theater projectionist accidentally splices a picture of ice cream into a movie so that it appears for the same amount of time as the alleged subliminal advertising Crisp discusses. If someone in the audience buys ice cream due to this influence, is their choice autonomous on Crisp's account? (Hint'\",\n",
    " \"Write your own reading comprehension question for students in next year's class. Explain the correct answer.\"]\n",
    "\n",
    "drop = ['name','student_id', 'sis_id', 'section', 'section_id', 'section_sis_id',\n",
    "       'submitted', 'attempt','n correct', 'n incorrect', 'score', 'course_id', 'quiz_id',\n",
    "       'submission_id', 'graded_total', 'level_0', 'index']\n",
    "# d.reset_index(inplace=True)\n",
    "c = d.drop(drop, axis=1)\n",
    "\n",
    "c =c.rename(lambda x : x[:50].strip(), axis=1)\n",
    "c\n",
    "\n",
    "bgs = [(f, make_bag_from_oa_field(c, f)) for f in c]\n",
    "    \n",
    "\n",
    "for title,bag in bgs:\n",
    "    draw_cloud(bag, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attic\n",
    "\n",
    "These cells are probably ready for deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "WAIT =5\n",
    "NO_LATE_PENALTY =True\n",
    "\n",
    "\n",
    "# programmatically get data\n",
    "activity = [c for c in environment.CONFIG.unit.components if c.id == environment.CONFIG.assignments[0][0]][0]\n",
    "    \n",
    "if NO_LATE_PENALTY:\n",
    "    activity.due_at = activity.lock_at\n",
    "\n",
    "workRepo = WorkRepositoryLoaderFactory.make(course=environment.CONFIG.course, activity=activity, rest_timeout=WAIT)\n",
    "\n",
    "quiz = environment.CONFIG.course.get_quiz(activity.quiz_id)\n",
    "subRepo = QuizSubmissionRepository(quiz)\n",
    "\n",
    "# Filter previously graded\n",
    "subRepo.data = [s for s in subRepo.data if s.workflow_state != 'complete']\n",
    "workRepo.data = workRepo.data[workRepo.data.workflow_state != 'complete'].copy(deep=True)\n",
    "\n",
    "graded = []\n",
    "\n",
    "workRepo.data.reset_index(inplace=True)\n",
    "# Let the grader do its job\n",
    "grader = QuizGrader(workRepo, subRepo)\n",
    "g = grader.grade(on_empty=0)\n",
    "graded += g\n",
    "print(\"Graded: \", len(graded))\n",
    "\n",
    "# If this is a quiz type assignment, we need to get the quiz submission object\n",
    "# not the regular submission object\n",
    "\n",
    "quiz = environment.CONFIG.course.get_quiz(activity.quiz_id)\n",
    "qsubs = [s for s in quiz.get_submissions()]\n",
    "\n",
    "def get_submission_object(student_id, attempt):    \n",
    "    return [d for d in qsubs if d.user_id == student_id and d.attempt == attempt]\n",
    "\n",
    "# Upload grades\n",
    "for g in graded:\n",
    "    sub = get_submission_object(g['student_id'], g['attempt'])[0]\n",
    "    sub.update_score_and_comments(quiz_submissions=g['data']['quiz_submissions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# Version prior to 2020-02-16\n",
    "# This handles everything\n",
    "# Make sure to manually download the reports and\n",
    "# to download the submission info before running\n",
    "\n",
    "GRACE_PERIOD = pd.Timedelta('2 days')\n",
    "\n",
    "graded = []\n",
    "for quiz in quizzes:\n",
    "    print(\"Grading: {}\".format(quiz.name))\n",
    "    # Load all student data for selected assignments from csv files\n",
    "    fiter = makeDataFileIterator(quiz.folder_path)\n",
    "    try:\n",
    "        while True:\n",
    "            f = next(fiter)\n",
    "            print(\"loading: \", f)\n",
    "            student_work = load_student_work(f, submissions)\n",
    "\n",
    "            # add submission info and use to remove non-final attempts\n",
    "            remove_non_final_attempts(student_work)\n",
    "            # Add some values to the quiz before grading\n",
    "            quiz.quarter_credit_date = QUARTER_CREDIT_DATE\n",
    "            quiz.set_question_columns(student_work)\n",
    "            # Grade it\n",
    "            g = grade(student_work, quiz, GRACE_PERIOD)\n",
    "            graded.append(g)\n",
    "            # Save copy to log folder\n",
    "            save_json(g, quiz)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "len(graded[0])\n",
    "\n",
    "# graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from CanvasHacks.Repositories.quizzes import process_work, load_student_work, remove_non_final_attempts, make_drop_list, save_json, drop_columns_from_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "assignments = []\n",
    "for quiz in quizzes:\n",
    "# for folder_path in ASSIGNMENT_FOLDERS:\n",
    "#     print(quiz.folder_path)\n",
    "    fiter = makeDataFileIterator(quiz.folder_path)\n",
    "    try:\n",
    "        while True:\n",
    "            f = next(fiter)\n",
    "            print(f)\n",
    "            frame = load_student_work(f, submissions)\n",
    "            assignments.append(frame)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "len(assignments)\n",
    "\n",
    "# Load all student data for selected assignments from csv files\n",
    "assignments = []\n",
    "for quiz in quizzes:\n",
    "# for folder_path in ASSIGNMENT_FOLDERS:\n",
    "    print(\"Quiz: \", quiz.folder_path)\n",
    "    fiter = makeDataFileIterator(quiz.folder_path)\n",
    "#     frame = load_student_work(next(fiter), submissions)\n",
    "    try:\n",
    "        while True:\n",
    "            f = next(fiter)\n",
    "            print(\"loading: \", f)\n",
    "            frame = load_student_work(f, submissions)\n",
    "            # add submission info and use to remove non-final attempts\n",
    "            remove_non_final_attempts(frame)\n",
    "            assignments.append(frame)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    \n",
    "#     assignments.append(frame)\n",
    "len(assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "course_id = sections[0]\n",
    "\n",
    "quiz_id = environment.CONFIG.assignments[0][0]\n",
    "results = get_all_quiz_submissions(course_id, quiz_id)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_responses(row, question_columns):\n",
    "    score = 0\n",
    "    for c in question_columns:\n",
    "        try:\n",
    "            if pd.isnull(row[c]): \n",
    "                raise Exception\n",
    "                \n",
    "            # No credit for 1 word answers\n",
    "            if len(row[c]) < 2:\n",
    "                raise Exception\n",
    "            \n",
    "            # If we made it past the tests, increment the score\n",
    "            score += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "                \n",
    "    return score\n",
    "\n",
    "def add_graded_total_field(frame, question_columns):\n",
    "    frame['graded_total'] = frame.apply(lambda r: check_responses(r, question_columns), axis=1)\n",
    "\n",
    "\n",
    "def save_to_log_folder(frame):\n",
    "    section = frame.iloc[0]['section']\n",
    "    qid = frame.iloc[0]['quiz_id']\n",
    "    fname = \"%s/%s-%s-results.xlsx\" % (environment.LOG_FOLDER, section, qid)\n",
    "    print(\"Saving to \", fname)\n",
    "    frame.to_excel(fname)\n",
    "\n",
    "def grade(frame, quiz_data_obj, grace_period=None):\n",
    "    \"\"\"\n",
    "    This handles the actual grading\n",
    "    \n",
    "    quiz_data_obj will have the payload format:\n",
    "        \"quiz_submissions\": [{\n",
    "        \"attempt\": int(attempt),\n",
    "        \"fudge_points\": total_score\n",
    "      },\n",
    "          \"questions\": {\n",
    "      \"QUESTION_ID\": {\n",
    "        \"score\": null, // null for no change, or an unsigned decimal\n",
    "        \"comment\": null // null for no change, '' for no comment, or a string\n",
    "      }\n",
    "    \"\"\"\n",
    "    results = []\n",
    "#     questions = detect_question_columns(frame.columns)\n",
    "\n",
    "    for i, row in frame.iterrows():\n",
    "        fudge_points = 0\n",
    "        out = {\n",
    "            'student_id': int(row['student_id']),\n",
    "            'attempt': int(row['attempt']),\n",
    "            'submission_id': int(row['submission_id']),\n",
    "            'course_id': int(row['course_id']),\n",
    "            'quiz_id': int(row['quiz_id']),\n",
    "            'data': {}\n",
    "        }\n",
    "        # used for computing penalty \n",
    "        total_score = 0\n",
    "        questions = {}\n",
    "\n",
    "        for qid, column_name in quiz_data_obj.question_columns:\n",
    "            if pd.isnull(row[column_name]):\n",
    "                questions[qid] = { 'score': 0}\n",
    "            else:\n",
    "                questions[qid] = { 'score': 1.0 }\n",
    "                total_score += 1\n",
    "        \n",
    "        # compute penalty if needed\n",
    "        penalty = get_penalty(row['submitted'], quiz_data_obj.due_date, quiz_data_obj.quarter_credit_date, grace_period)\n",
    "        # will be 0 if not docking\n",
    "        fudge_points = total_score * -penalty\n",
    "        if penalty >0:\n",
    "            print('Student #{}: Submitted on {}; was due {}. Penalized {}'.format(row['student_id'], row['submitted'],quiz_data_obj.due_date, penalty))\n",
    "\n",
    "\n",
    "        out['data'][\"quiz_submissions\"] = [\n",
    "            { \n",
    "                \"attempt\": int(row['attempt']),\n",
    "                \"fudge_points\": fudge_points,\n",
    "                \"questions\": questions\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        results.append(out)\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_json(grade_data, quiz_data_obj):\n",
    "    fpath = \"%s/%s-%s-all-submissions.json\" % (environment.LOG_FOLDER, quiz_data_obj.course_id, quiz_data_obj.id)\n",
    "    # save submissions\n",
    "    with open(fpath, 'w') as fpp:\n",
    "        json.dump(grade_data, fpp)\n",
    "\n",
    "\n",
    "def upload_quiz_grades(grade_data):\n",
    "    cnt = 0\n",
    "    for r in grade_data:\n",
    "        url = make_url(r['course_id'], 'quizzes')\n",
    "        url += \"/%s/submissions/%s\" % (r['quiz_id'], r['submission_id'])\n",
    "#         print(url)\n",
    "        response = send_put_request(url, r['data'])\n",
    "        cnt += 1\n",
    "    print(\"{} grades uploaded\".format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuizGrader:\n",
    "\n",
    "    def __init__( self, work_repo, submission_repo, grade_func=None):\n",
    "        \"\"\"\n",
    "        :param grade_func: Function or method to use to determine grade\n",
    "        \"\"\"\n",
    "        self.grade_func = grade_func\n",
    "        self.work_repo = work_repo\n",
    "        self.submission_repo = submission_repo\n",
    "\n",
    "\n",
    "    @property\n",
    "    def activity( self ):\n",
    "        return self.work_repo.activity\n",
    "\n",
    "    def grade( self ):\n",
    "        self.graded = [ ]\n",
    "        for i, row in self.work_repo.data.iterrows():\n",
    "            self.graded.append(self._grade_row(row))\n",
    "        return self.graded\n",
    "\n",
    "    def _grade_row(self, row):\n",
    "        fudge_points = 0\n",
    "        out = {\n",
    "            'student_id': int( row[ 'student_id' ] ),\n",
    "            'attempt': int( row[ 'attempt' ] ),\n",
    "            'submission_id': int( row[ 'submission_id' ] ),\n",
    "            'course_id': int( row[ 'course_id' ] ),\n",
    "            'quiz_id': int( row[ 'quiz_id' ] ),\n",
    "            'data': { }\n",
    "        }\n",
    "        # used for computing penalty\n",
    "        total_score = 0\n",
    "        questions = { }\n",
    "\n",
    "        # Grade on emptiness\n",
    "        # todo This should use credit_no_credit from GradingTools.nonempty\n",
    "        for qid, column_name in self.work_repo.question_columns:\n",
    "            if pd.isnull( row[ column_name ] ):\n",
    "                questions[ qid ] = { 'score': 0 }\n",
    "            else:\n",
    "                questions[ qid ] = { 'score': 1.0 }\n",
    "                total_score += 1\n",
    "\n",
    "        # compute penalty if needed\n",
    "        penalty = get_penalty( row[ 'submitted' ], self.activity.due_at, self.activity.lock_at, self.activity.grace_period )\n",
    "        # will be 0 if not docking\n",
    "        fudge_points = total_score * -penalty\n",
    "        if penalty > 0:\n",
    "            print(self._penalty_message( penalty, row ))\n",
    "\n",
    "        out[ 'data' ][ \"quiz_submissions\" ] = [\n",
    "            {\n",
    "                \"attempt\": int( row[ 'attempt' ] ),\n",
    "                \"fudge_points\": fudge_points,\n",
    "                \"questions\": questions\n",
    "            }\n",
    "        ]\n",
    "        return out\n",
    "\n",
    "    def _penalty_message( self, penalty, row ):\n",
    "        stem = 'Student #{}: Submitted on {}; was due {}. Penalized {}'\n",
    "        return stem.format( row[ 'student_id' ], row[ 'submitted' ], self.activity.due_at, penalty )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# We'll need the id rather than the assignment_id for quizzes, so \n",
    "# we need to replace the assignment ids in the store\n",
    "quizzes = get_all_course_quizzes(sections[0])\n",
    "\n",
    "n = []\n",
    "for a_id, name in environment.CONFIG.assignments:\n",
    "    for q in quizzes:\n",
    "        if q['title'] == name:\n",
    "            n.append((q['id'], name))\n",
    "#             print(a_id, q['id'], name)\n",
    "\n",
    "environment.CONFIG.assignments = n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_quiz_questions(course_id, quiz_id):\n",
    "    \"\"\"\n",
    "    GET /api/v1/courses/:course_id/quizzes/:quiz_id/questions \n",
    "    \"\"\"\n",
    "    url = make_url(course_id, 'quizzes')\n",
    "    url += \"/%s/questions\" % quiz_id\n",
    "    print(url)\n",
    "    return send_get_request(url)\n",
    "\n",
    "questions = get_quiz_questions(41179, 100843)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_due_date(quiz_id, quizzes):\n",
    "    return pd.to_datetime(list(filter(lambda x: x['id'] == quiz_id, quizzes))[0]['due_at'])\n",
    "\n",
    "def get_lock_date(quiz_id, quizzes):\n",
    "    return pd.to_datetime(list(filter(lambda x: x['id'] == quiz_id, quizzes))[0]['lock_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# This stuff was earlier version which didn't work\n",
    "# The working scripts to grade and upload are below in next section\n",
    "# Keeping for now, because may be good to use this for backup of data\n",
    "\n",
    "for frame in frames:\n",
    "    questions = detect_question_columns(frame.columns)\n",
    "    add_graded_total_field(frame, questions)\n",
    "    remove_non_final_attempts(frame)\n",
    "#     save_to_log_folder(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def add_penalty(data, max_possible, penalty):\n",
    "    \"\"\"data should look like this:\n",
    "    {'student_id': 1234,\n",
    " 'attempt': 1,\n",
    " 'submission_id': 12345,\n",
    " 'course_id': 12345,\n",
    " 'quiz_id': 12345,\n",
    " 'data': {'quiz_submissions': [{'attempt': 1,\n",
    "    'fudge_points': 0,\n",
    "    'questions': {'2044306': {'score': 1.0},\n",
    "     '2282600': {'score': 1.0},\n",
    "     '2044307': {'score': 1.0},\n",
    "     '2044308': {'score': 1.0},\n",
    "     '2044309': {'score': 1.0}}}]}}\n",
    "\n",
    "\"\"\"\n",
    "    data['data']['quiz_submissions'][0]['fudge_points'] = -max_possible * penalty#['questions']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canv-env",
   "language": "python",
   "name": "canv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "906.8px",
    "left": "0px",
    "right": "969.8px",
    "top": "110.2px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
