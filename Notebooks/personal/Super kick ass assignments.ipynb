{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ideas for how to make this work__\n",
    "\n",
    "- First assignment should be a dummy, but still graded use\n",
    "\n",
    "- Suggest that students open canvas in 2 tabs/windows so can see original\n",
    "\n",
    "- Video walk through of an assignment\n",
    "\n",
    "- Have a checkbox before turn in any feedback that says \"I have read my response pretending that I am the receipient and believe that the feedback is accurate, fair, and presented with [kindness? empathy?]\"\n",
    "\n",
    "- Tell that this is work in progress. Precise proportions of each grade may vary (e.g., amount for completion vs assigned). Always will be with an eye to students who complete everything pass\n",
    "\n",
    "- My feedback / examples etc should just be loaded into the feedback they see when they turn in the first assigment\n",
    "\n",
    "- Look through mdp materials for articles to read on constructive feedback\n",
    "\n",
    "\n",
    "\n",
    "- Student assigned grades should be in terms of A, B, C, D, F. I will determine how to translate into points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://canvas.instructure.com/doc/api/peer_reviews.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/Dropbox/CanvasHacks\n",
    "\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "import datetime\n",
    "\n",
    "from CanvasHacks import environment\n",
    "# from CanvasHacks.RequestTools import *\n",
    "# from CanvasHacks.UrlTools import *\n",
    "from CanvasHacks.Configuration import InteractiveConfiguration\n",
    "import CanvasHacks.GradingTools as GT\n",
    "import CanvasHacks.DownloadProcessingTools as PT\n",
    "\n",
    "# File system\n",
    "# This aren't used in the non-saving version\n",
    "from CanvasHacks.FileTools import makeDataFileIterator, create_folder\n",
    "from CanvasHacks.TimeTools import getDateForMakingFileName\n",
    "from CanvasHacks.QuizReportFileTools import load_new\n",
    "\n",
    "\n",
    "# Canvas api\n",
    "from canvasapi import Canvas\n",
    "from canvasapi.quiz import QuizReport, Quiz\n",
    "from canvasapi.requester import Requester\n",
    "from canvasapi.conversation import Conversation\n",
    "\n",
    "# Initialize a Canvas api objects\n",
    "canvas = Canvas(environment.CONFIG.canvas_url_base, environment.CONFIG.canvas_token)\n",
    "requester = Requester(environment.CONFIG.canvas_url_base, environment.CONFIG.canvas_token)\n",
    "\n",
    "# Configuration\n",
    "from CanvasHacks.PeerReviewed.Definitions import Review, InitialWork, MetaReview, Unit #Assignment\n",
    "\n",
    "# Exceptions\n",
    "from CanvasHacks.Errors.review_pairings import AlreadyAssigned, SubmissionIncomplete\n",
    "\n",
    "# Models\n",
    "from CanvasHacks.Models.student import Student\n",
    "from CanvasHacks.Models.student import student_from_canvas_user, ensure_student\n",
    "\n",
    "# Notifications\n",
    "# from CanvasHacks.PeerReviewed.Notifications import notify_student\n",
    "# from CanvasHacks.Messaging.Messengers import make_prompt_and_response, make_notice, metareview_send_message_to_reviewers, review_send_message_to_reviewers\n",
    "from CanvasHacks.Models.student import get_first_name\n",
    "\n",
    "# Plotting\n",
    "from CanvasHacks.VisualizationTools import rotate_x_labels\n",
    "\n",
    "# Repos\n",
    "from CanvasHacks.Repositories.DataManagement import DataStore\n",
    "from CanvasHacks.Repositories.quizzes import QuizRepository, ReviewRepository\n",
    "from CanvasHacks.Repositories.codes import AccessCodeRepo\n",
    "from CanvasHacks.Logging.review_pairings import make_review_audit_file\n",
    "from CanvasHacks.Repositories.reviewer_associations import assign_reviewers, AssociationRepository\n",
    "from CanvasHacks.Repositories.students import StudentRepository\n",
    "from CanvasHacks.Repositories.submissions import SubmissionRepository, QuizSubmissionRepository\n",
    "\n",
    "# Storage\n",
    "from CanvasHacks.DAOs.sqlite_dao import SqliteDAO\n",
    "\n",
    "# Widgets\n",
    "from CanvasHacks.Widgets.ConsolidatedTextOutput import make_assignment_header, make_consolidated_text_fields\n",
    "from CanvasHacks.Widgets.InputFields import make_course_ids_input, make_canvas_token_input, make_canvas_url_input, make_general_reset_button\n",
    "from CanvasHacks.Widgets.AssignmentSelection import make_assignment_chooser, view_selected_assignments, view_ungraded_assignments\n",
    "from CanvasHacks.UploadGradeTools import make_upload_button\n",
    "from CanvasHacks.Widgets.LiveSelection import make_test_selector\n",
    "from CanvasHacks.Widgets.AssignmentSelection import make_unit_chooser\n",
    "\n",
    "# Main things\n",
    "from CanvasHacks.SkaaSteps.SendInitialWorkToReviewer import SendInitialWorkToReviewer\n",
    "from CanvasHacks.SkaaSteps.SendReviewToReviewee import SendReviewToReviewee\n",
    "from CanvasHacks.SkaaSteps.SendMetareviewToReviewer import SendMetareviewToReviewer\n",
    "\n",
    "from CanvasHacks.Models.status_record import StatusRecord\n",
    "\n",
    "import inspect\n",
    "def look_inside(obj):\n",
    "    print(inspect.getmembers(obj, lambda a:not(inspect.isroutine(a))))\n",
    "\n",
    "    \n",
    "SEMESTER_NAME = 'S20'\n",
    "LOC = '{}/Box Sync/TEACHING/Phil 305 Business ethics/Phil305 S20'.format(environment.ROOT)# placeholder for where the access codes are stored\n",
    "ACCESS_CODES_FP = \"{}/{}-assignment-access-codes.xlsx\".format(LOC, SEMESTER_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import CanvasHacks.testglobals\n",
    "# Set on the global variable (which is only used in certain tests)\n",
    "# CanvasHacks.testglobals.TEST\n",
    "CanvasHacks.testglobals.TEST_WITH_FILE_DB = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "make_test_selector()\n",
    "make_unit_chooser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SEND = True\n",
    "step1 = SendInitialWorkToReviewer(course=environment.CONFIG.course, unit=environment.CONFIG.unit, send=SEND)\n",
    "step1.run(rest_timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.CONFIG.unit.metareview.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Distribute peer reviews to authors and request to do metareview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1.associationRepo.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e =[]\n",
    "for r in step1.associationRepo.data:\n",
    "#     print(step1.studentRepo.get_student(r.assessor_id).__dict__)\n",
    "    e.append(step1.studentRepo.get_student(r.assessor_id))\n",
    "len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = step1.work_repo.data[step1.work_repo.data.workflow_state != 'unsubmitted']\n",
    "d = step1.work_repo.data[~pd.isnull(step1.work_repo.data.body)]\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1.studentRepo.get_student(170579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SEND = True\n",
    "step2 = SendReviewToReviewee(environment.CONFIG.course, environment.CONFIG.unit, send=SEND)\n",
    "step2.run(rest_timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from CanvasHacks.Models.status_record import StatusRecord\n",
    "\n",
    "review_invites = step2.dao.session.query( StatusRecord ).filter( StatusRecord.activity_id == environment.CONFIG.unit.review.id).all()\n",
    "for m in review_invites:\n",
    "    print(m.__dict__)\n",
    "review_invites\n",
    "\n",
    "metareview_invites = step2.dao.session.query( StatusRecord ).filter( StatusRecord.activity_id == environment.CONFIG.unit.metareview.id).all()\n",
    "for m in metareview_invites:\n",
    "    print(m.__dict__)\n",
    "\n",
    "init = step2.dao.session.query( StatusRecord ).filter( StatusRecord.activity_id == environment.CONFIG.unit.initial_work.id).all()\n",
    "init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return feedback from metareview to the person who did the peer review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEND = False\n",
    "step3 = SendMetareviewToReviewer(environment.CONFIG.course, environment.CONFIG.unit, send=SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3.run(rest_timeout=5, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3.notificationStatusRepo.previously_sent_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3.studentRepo.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3.dao.session.query(StatusRecord).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3.notificationStatusRepo.previously_sent_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3.work_repo.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metareview_snt = step3.dao.session.query( StatusRecord ).filter( StatusRecord.activity_id == environment.CONFIG.unit.metareview.id).all()\n",
    "metareview_snt\n",
    "\n",
    "for m in metareview_snt:\n",
    "    print(m.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Run after the unit has been selected\n",
    "canvas = environment.CONFIG.canvas\n",
    "course = environment.CONFIG.course\n",
    "unit = environment.CONFIG.unit\n",
    "\n",
    "codeRepo = AccessCodeRepo(ACCESS_CODES_FP, environment.CONFIG.unit)\n",
    "\n",
    "studentRepo = StudentRepository(environment.CONFIG.course)\n",
    "studentRepo.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data for content assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from CanvasHacks.QuizReportFileTools import  make_quiz_repo\n",
    "\n",
    "# from CanvasHacks.QuizReportFileTools import retrieve_quiz_data, save_downloaded_report, make_quiz_repo\n",
    "\n",
    "# def make_quiz_repo( course, activity, save=True):\n",
    "#     \"\"\"Gets all student work data for the activity that's part of the assignment\n",
    "#     loads it into a QuizRepository or ReviewRepository and \n",
    "#     returns the repository\n",
    "#     \"\"\"\n",
    "#     # Get quiz submission objects\n",
    "#     if isinstance(activity, Review):\n",
    "#         repo = ReviewRepository(activity, course)\n",
    "#     else:\n",
    "#         repo = QuizRepository(activity, course)\n",
    "    \n",
    "#     # Download student work\n",
    "#     # This will work if the 'Create Report' button has been manually clicked\n",
    "#     student_work_frame = retrieve_quiz_data(repo.quiz)\n",
    "    \n",
    "#     if save:\n",
    "#         # Want to have all the reports be formatted the same\n",
    "#         # regardless of whether we manually or programmatically \n",
    "#         # downloaded them. Thus we save before doing anything to them.\n",
    "#         save_downloaded_report(activity, student_work_frame)\n",
    "\n",
    "#     # Download submissions\n",
    "#     subRepo = QuizSubmissionRepository(repo.quiz)\n",
    "\n",
    "#     # Doing the combination with submissions after saving to avoid \n",
    "#     # mismatches of new and old data\n",
    "#     repo._process(student_work_frame, subRepo.frame)\n",
    "    \n",
    "#     return repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from file like I'm an animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually from file like I'm an animal\n",
    "\n",
    "# create_folder(unit.initial_work.folder_path)\n",
    "\n",
    "from CanvasHacks.Repositories.quizzes import load_student_work\n",
    "\n",
    "# Load all student data for selected assignments from csv files\n",
    "assignments = []\n",
    "# for folder_path in ASSIGNMENT_FOLDERS:\n",
    "print(\"Quiz: \", )\n",
    "fiter = makeDataFileIterator(unit.initial_work.folder_path)\n",
    "#     frame = load_student_work(next(fiter), submissions)\n",
    "try:\n",
    "    while True:\n",
    "        f = next(fiter)\n",
    "        print(\"loading: \", f)\n",
    "        frame = pd.read_csv(f)\n",
    "#         frame = load_student_work(f, submissions)\n",
    "        # add submission info and use to remove non-final attempts\n",
    "#         remove_non_final_attempts(frame)\n",
    "        assignments.append(frame)\n",
    "except StopIteration:\n",
    "    pass\n",
    "\n",
    "student_work_frame = assignments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If loading previously processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Run this is loading processed data from file\n",
    "# Manually add student work\n",
    "initial_work_repo = QuizRepository(unit.initial_work)\n",
    "initial_work_repo.data = student_work_frame\n",
    "initial_work_repo.set_question_columns(initial_work_repo.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If loading for first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Run this if doing manually and getting data for first time\n",
    "course = canvas.get_course(unit.initial_work.course_id)\n",
    "\n",
    "quiz = course.get_quiz(unit.initial_work.quiz_id)\n",
    "\n",
    "# Download submissions\n",
    "submissions = quiz.get_submissions()    \n",
    "# the canvasapi has returned a bunch of objects\n",
    "# so we convert to dicts to make easier to create dataframe\n",
    "submissions = [l.__dict__ for l in list(submissions)]\n",
    "\n",
    "# Manually add student work\n",
    "initial_work_repo = QuizRepository(unit.initial_work)\n",
    "initial_work_repo._process(student_work_frame, submissions)\n",
    "len(initial_work_repo.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The right way: programmatic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way its supposed to be\n",
    "# Load data\n",
    "\n",
    "initial_work_repo = make_quiz_repo(environment.CONFIG.course, unit.initial_work)\n",
    "\n",
    "initial_work_repo.data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign peer reviewers\n",
    "__Carried out after the initial work due date__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on CAN-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No one assigned to assess them:\n",
    "    Artur Alaverdyan\n",
    "    Javier Parra\n",
    "    Joseph Pouldar\n",
    "    Kathryn Timmer\n",
    "    Saida Alexandra Khurram\n",
    "    \n",
    "Someone assigned to assess them them\n",
    "    Mohammad Karbalaei Ali Akbar --- Berenice\n",
    "    \n",
    "    \n",
    "No one assigned for them to assess:\n",
    "    Artur Alaverdyan\n",
    "    Javier Parra\n",
    "    Joseph Pouldar\n",
    "    Kathryn Timmer\n",
    "    Saida Alexandra Khurram\n",
    "    \n",
    "Someone assigned for them to assess:\n",
    "    Mohammad Karbalaei Ali Akbar ---- Sevada Keshishi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def make_column_map(question_columns):\n",
    "    return { question_columns[i] : 'q{}'.format(i+1) for i in range(0, len(question_columns)) }\n",
    "\n",
    "assert(make_column_map(['long1', 'long2']) == {'long1' : 'q1', 'long2': 'q2'})\n",
    "\n",
    "fp = \"{}/Desktop/TEMPORARY/Unit 1_CA.xlsx\".format(environment.ROOT)\n",
    "dt = pd.read_excel(fp)\n",
    "\n",
    "cmap = {'3031694: Explain the structure of a tragedy of the commons . Make sure you explain why it remains rational to cheat even when one knows that enough people cheating will result in catastrophe. Then explain 2 measures by which the villagers in the overfishi' : 'q1'}\n",
    "dt.rename(cmap, axis=1, inplace=True)\n",
    "dt.rename({'id' : 'student_id'}, axis=1, inplace=True)\n",
    "dt = dt[['name', 'student_id', 'sis_id', 'submitted', 'attempt', 'q1']]\n",
    "dt\n",
    "\n",
    "\n",
    "quiz = course.get_quiz(unit.initial_work.quiz_id)\n",
    "\n",
    "# Download submissions\n",
    "submissions = quiz.get_submissions()    \n",
    "# the canvasapi has returned a bunch of objects\n",
    "# so we convert to dicts to make easier to create dataframe\n",
    "submissions = [l.__dict__ for l in list(submissions)]\n",
    "\n",
    "submissions = pd.DataFrame(submissions)\n",
    "submissions.rename({'user_id' : 'student_id'}, inplace=True, axis=1)\n",
    "sub_cols = ['id', 'quiz_id', 'student_id', 'submission_id', \n",
    "       'end_at', 'end_at_date', 'finished_at', 'finished_at_date', 'attempt',\n",
    "       'workflow_state','has_seen_results','attempts_left', 'course_id']\n",
    "\n",
    "submissions = submissions[sub_cols]\n",
    "\n",
    "len(submissions)\n",
    "\n",
    "stups = submissions[['student_id', 'attempt', 'submission_id']]\n",
    "stups = {sid : (a, subid) for idx,sid, a, subid in stups.itertuples()}\n",
    "\n",
    "def add_sub_id(student_id, attempt, stups):\n",
    "    s = stups.get(student_id)\n",
    "    if s[0] == attempt:\n",
    "        return s[1]\n",
    "    return None\n",
    "\n",
    "dt['submission_id'] = dt.apply(lambda x: add_sub_id(x.student_id, x.attempt, stups), axis=1)\n",
    "dt\n",
    "\n",
    "filter(stups, lambda x: x[0] == 57180)\n",
    "\n",
    "def add_sub_id(student_id, attempt, stups):\n",
    "    s = stups.get(student_id)\n",
    "    if s[0] == attempt:\n",
    "        return s[1]\n",
    "    return None\n",
    "\n",
    "dt['submission_id'] = dt.apply(lambda x: add_sub_id(x.student_id, x.attempt, stups), axis=1)\n",
    "\n",
    "\n",
    "dt.dropna( subset=[ 'submission_id' ], inplace=True )\n",
    "len(dt)\n",
    "\n",
    "def local_dt_string_to_utc( local_string ):\n",
    "    \"\"\"'2020-02-07T07:59:59Z'\n",
    "    returns Timestamp('2020-02-06 23:59:59-0800', tz='US/Pacific')\n",
    "    \"\"\"\n",
    "    return pd.Timestamp( local_string, tz='US/Pacific' ).tz_convert( 'utc' )\n",
    "\n",
    "\n",
    "submissions.finished_at = pd.to_datetime(submissions.finished_at)\n",
    "\n",
    "last_ran = pd.Timestamp('2020-02-05T10:10', tz='US/Pacific')\n",
    "\n",
    "pending_subs = submissions[submissions.finished_at > last_ran]\n",
    "\n",
    "late = dt[dt.student_id.isin(pending_subs.student_id) ]\n",
    "\n",
    "late = late[late.student_id != 155461]\n",
    "late\n",
    "\n",
    "LATE_REVIEW_ID = 11111\n",
    "\n",
    "late_ass_repo =  AssociationRepository(dao, LATE_REVIEW_ID)\n",
    "\n",
    "# iewers to each submitter and store in db\n",
    "associationRepo.assign_reviewers( late.student_id.tolist())\n",
    "\n",
    "\n",
    "for sid in late.student_id.tolist():\n",
    "    print(sid, associationRepo.get_assessee(unit.review, sid), associationRepo.get_assessor(unit.review, sid))\n",
    "\n",
    "\n",
    "for sid in pending_subs.student_id.tolist():\n",
    "    print(sid, associationRepo.get_assessee(unit.review, sid))\n",
    "    print(sid, associationRepo.get_assessor(unit.review, sid))\n",
    "\n",
    "\n",
    "\n",
    "dt.student_id.isin(pending_subs.student_id)\n",
    "\n",
    "review_assigns = associationRepo.get_associations(unit.review)\n",
    "review_assigns\n",
    "\n",
    "len(assignments[0])\n",
    "old = assignments[0]\n",
    "old.columns\n",
    "\n",
    "len(assignments[1])\n",
    "new = assignments[1]\n",
    "new.columns\n",
    "\n",
    "new[new.student_id not in old.student_id]\n",
    "\n",
    "a = pd.concat(assignments, sort=True)\n",
    "\n",
    "a[:3].T\n",
    "\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from CanvasHacks.SkaaSteps.SendInitialWorkToReviewer import SendInitialWorkToReviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SendInitialWorkToReviewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make and store reviewer assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if environment.CONFIG.is_test:\n",
    "    # testing: in memory db\n",
    "    dao = SqliteDAO()\n",
    "    print(\"Connected to testing db\")\n",
    "else:\n",
    "    db_filepath = \"{}/{}-Unit-{}-review-assigns.db\".format( environment.LOG_FOLDER, SEMESTER_NAME, environment.CONFIG.unit_number)\n",
    "    # real: file db\n",
    "    dao = SqliteDAO(db_filepath)\n",
    "    dao.initialize_db_file()\n",
    "    print(\"Connected to REAL db\")\n",
    "\n",
    "associationRepo = AssociationRepository(dao, unit.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_review_audit_file(associationRepo, unit):\n",
    "    \"\"\"Stores all review assignments to a csv file\n",
    "    for easier auditing\n",
    "    \"\"\"\n",
    "    # Make audit file\n",
    "    review_audit = []\n",
    "    for rev in associationRepo.get_associations(unit.review):\n",
    "        print(rev.assessor_id, rev.assessee_id)\n",
    "        assessor = studentRepo.get_student(rev.assessor_id)\n",
    "        assessee = studentRepo.get_student(rev.assessee_id)\n",
    "        print(assessor)\n",
    "\n",
    "        review_audit.append({\n",
    "            'activity' : unit.review.name,\n",
    "            'assessor_name' : assessor.short_name,\n",
    "            'assessor_sis_id': assessor.sis_user_id,\n",
    "            'assessor_canvas_id': assessor.id,\n",
    "            'assessee_name' : assessee.short_name,\n",
    "            'assessee_sis_id': assessee.sis_user_id,\n",
    "            'assessee_canvas_id': assessee.id,\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    review_audit = pd.DataFrame(review_audit)\n",
    "\n",
    "    fp = \"{}/{}-Unit{}-peer review assignments.csv\".format(environment.LOG_FOLDER, getDateForMakingFileName(), environment.CONFIG.unit )\n",
    "    review_audit.to_csv(fp)\n",
    "    print(\"Audit file saved to {}\".format(fp))\n",
    "    # review_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# all students who submitted the assignment\n",
    "# submitters = [169908, 169955, 169957]\n",
    "\n",
    "# submitters = list(set(initial_work_repo.data.reset_index().student_id.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign reviewers to each submitter and store in db\n",
    "associationRepo.assign_reviewers( initial_work_repo.submitter_ids)\n",
    "\n",
    "if not environment.CONFIG.is_test:\n",
    "    make_review_audit_file(associationRepo, environment.CONFIG.unit )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send initial work and instructions to reviewers\n",
    "\n",
    "Backup plan if can't use canvas conversations is to just use email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.PeerReviewed.Notifications import StudentWorkForPeerReviewMessenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEND = False\n",
    "# SEND = True\n",
    "msgr = StudentWorkForPeerReviewMessenger(unit.review, studentRepo, initial_work_repo )\n",
    "msgr.notify(associationRepo.data, SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SEND = False\n",
    "# SEND = True\n",
    "\n",
    "review_send_message_to_reviewers(associationRepo.data, studentRepo, initial_work_repo, unit.review, SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "associationRepo.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metareview\n",
    "\n",
    "__Carried out after the review due date__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download student work on review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL VERSION\n",
    "\n",
    "# Get work\n",
    "review_repo = make_quiz_repo(environment.CONFIG.course, unit.review)\n",
    "#review_repo.course = course\n",
    "review_repo._fix_forgot_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# FROM FILE TROUBLESHOOTING VERSION\n",
    "# review_repo\n",
    "# review_repo.data.to_csv('/Users/adam/Desktop/TEMPORARY/unit1peerreview.csv')\n",
    "review_repo = ReviewRepository(unit.review, course)\n",
    "review_repo.data = pd.read_csv('/Users/adam/Desktop/TEMPORARY/unit1peerreview.csv')\n",
    "review_repo.set_question_columns(review_repo.data)\n",
    "review_repo._fix_forgot_answers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For me\n",
    "rows = nrows=round(len(multiple_choice_names)/2)\n",
    "fig, axes = plt.subplots(ncols=2, nrows=rows, figsize=(12,20))\n",
    "\n",
    "row=0; col=0\n",
    "order = ['Forgot', 'Strongly disagree', 'Disagree', 'Agree', 'Strongly agree']\n",
    "for c in multiple_choice_names:\n",
    "    title = c.split(':')[1][:65]\n",
    "    g = sns.countplot(d[c], order=environment.LIKERT_PLOT_ORDER, palette='plasma', ax=axes[row, col])\n",
    "    g.set_xlabel(title)\n",
    "    rotate_x_labels(axes[row, col])\n",
    "    if col == 1:\n",
    "        row += 1\n",
    "        col = 0\n",
    "    else:\n",
    "        col += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "rows = len(review_repo.multiple_choice_names)\n",
    "fig, axes = plt.subplots(nrows=rows, figsize=(8,50))\n",
    "\n",
    "row=0; col=0\n",
    "for c in review_repo.multiple_choice_names:\n",
    "    title = c.split(':')[1][:65]\n",
    "    g = sns.countplot(d[c], order=environment.LIKERT_PLOT_ORDER, palette='plasma', ax=axes[row])\n",
    "    g.set_xlabel(title)\n",
    "    rotate_x_labels(axes[row])\n",
    "    row +=1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notify reviewed student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.PeerReviewed.Notifications import FeedbackForMetareviewMessenger\n",
    "\n",
    "SEND = False\n",
    "# SEND = True\n",
    "\n",
    "msgr = FeedbackForMetareviewMessenger(unit.metareview, studentRepo, review_repo )\n",
    "msgr.notify(associationRepo.data, SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SEND = False\n",
    "# SEND =True\n",
    "\n",
    "# Do this separately because the relevant activity is the previous review\n",
    "\n",
    "# Load list of ReviewAssociation objects representing who reviews whom\n",
    "# review_assigns = associationRepo.get_associations(unit.review)\n",
    "print(\"loaded {} student reviewer assignments\".format(len(review_assigns)))\n",
    "\n",
    "\n",
    "metareview_send_message_to_reviewers(associationRepo.data, studentRepo, review_repo, unit.metareview, SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send late peer review submissions out for metareview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newframe = load_new(unit.review)\n",
    "\n",
    "review_repo = ReviewRepository(unit.review, course)\n",
    "review_repo.data = newframe\n",
    "review_repo.set_question_columns(review_repo.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEND = False\n",
    "# SEND = True\n",
    "\n",
    "msgr = FeedbackForMetareviewMessaging(unit.metareview, studentRepo, review_repo )\n",
    "msgr.notify(associationRepo.data, SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SEND = False\n",
    "# SEND =True\n",
    "\n",
    "# Load list of ReviewAssociation objects representing who reviews whom\n",
    "# Do this separately because the relevant activity is the previous review\n",
    "review_assigns = associationRepo.get_associations(unit.review)\n",
    "# prune to just the new stuff\n",
    "review_assigns = [ r for r in review_assigns if r.assessor_id in review_repo.data.index]\n",
    "print(\"loaded {} student reviewer assignments\".format(len(review_assigns)))\n",
    "\n",
    "metareview_send_message_to_reviewers(review_assigns, studentRepo, review_repo, unit.metareview, SEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing out\n",
    "\n",
    "__Carried out after the meta review due date__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign grades \n",
    "\n",
    "based partially on review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign grades\n",
    "\n",
    "Grade review for completion\n",
    "\n",
    "Apply grades given in review to initial submitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent at 2020-02-09T22:49:59Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metareview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send metareview feedback and stats to reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the feedback given in the metareview\n",
    "\n",
    "# Calculate a statistical summary of how the ratings they gave vs. class\n",
    "\n",
    "# Calculate a statistical summary of ratings received vs class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_repo = make_quiz_repo(assignment.meta_review)\n",
    "# Grade metareview\n",
    "\n",
    "# Apply grades given in metareview to reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload  grades\n",
    "\n",
    "(This may not be necessary if metareview is created via a survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical and other summary information for everyone\n",
    "\n",
    "We upload as much anonymized info as possible so that everyone can see scoring is working. This is to help everyone improve and anchor standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cron job that handles all of this automagically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define based on Assignment and set running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abberant cases\n",
    "\n",
    "- Assigned reviewer does not turn in review\n",
    "\n",
    "- Reviewd student does not turn in metareview\n",
    "\n",
    "- Odd number of students turn in\n",
    "\n",
    "- Incomplete submissions\n",
    "\n",
    "- Multiple discsussion submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#Auditing\n",
    "\n",
    "review_assigns = associationRepo.get_associations(unit.review)\n",
    "# review_assigns\n",
    "\n",
    "len(review_assigns)\n",
    "\n",
    "log = pd.read_csv(\"{}/2020-02-05-Unit1-peer review assignments.csv\".format(environment.LOG_FOLDER))\n",
    "\n",
    "log.set_index('assessor_canvas_id', inplace=True)\n",
    "log\n",
    "\n",
    "for ra in review_assigns:\n",
    "    try:\n",
    "        row = log.loc[ra.assessor_id]\n",
    "        if ra.assessee_id != row.assessee_canvas_id:\n",
    "            print(\"Log assessor {} assessee {} | DB assessee {}\".format(ra.assessor_id, ra.assessee_id, row.assessee_canvas_id, ))\n",
    "    except KeyError:\n",
    "        print(\"DB assessor id not in log: \", ra.assessor_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT\n",
    "\n",
    "log = pd.read_csv(\"{}/2020-02-05-Unit1-peer review assignments.csv\".format(environment.LOG_FOLDER))\n",
    "\n",
    "log.set_index('assessor_canvas_id', inplace=True)\n",
    "log\n",
    "\n",
    "for ra in review_assigns:\n",
    "    try:\n",
    "        row = log.loc[ra.assessor_id]\n",
    "        if ra.assessee_id != row.assessee_canvas_id:\n",
    "            print(\"Log assessor {} assessee {} | DB assessee {}\".format(ra.assessor_id, ra.assessee_id, row.assessee_canvas_id, ))\n",
    "    except KeyError:\n",
    "        print(\"DB assessor id not in log: \", ra.assessor_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Various test data stores\n",
    "\n",
    "testfileloc = \"{}/Desktop/TEMPORARY/test.csv\".format(environment.ROOT)\n",
    "test_data_folder = \"{}/tests/testdata/\".format(environment.CONFIG.proj_base)\n",
    "test_quiz_results = \"{}/quiz results.csv\".format(test_data_folder)\n",
    "test_submissions_frame = \"{}/test submissions.csv\".format(test_data_folder)\n",
    "# this is what's stored in QuizRepo.data for the initial submisison after processing\n",
    "test_combined_frame =\"{}/test combined.csv\".format(test_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "adam_id = 6417\n",
    "\n",
    "# Test data\n",
    "test_course_id = 85210\n",
    "\n",
    "main_quiz = 165098\n",
    "review_quiz = 165820\n",
    "meta_review = 165821\n",
    "\n",
    "url = 'https://canvas.csun.edu/api/v1/courses/85210/assignments/{}/submissions'\n",
    "\n",
    "# last semester data\n",
    "prev_course_id = 62657\n",
    "prev_quiz = 151633\n",
    "\n",
    "COURSE_ID = test_course_id\n",
    "# COURSE_ID = prev_course_id\n",
    "# QUIZ_ID = prev_quiz\n",
    "\n",
    "TEST_STUDENT_ID = 168439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_repo.data.set_index('student_id', inplace=True)\n",
    "\n",
    "unit.metareview.access_code = ''\n",
    "\n",
    "pd.read_csv('/Users/adam/Box Sync/TEACHING/Phil 305 Business ethics/Student work/77522-Unit 1 Peer review/2020-02-08-Unit 1 Peer review-student-work.csv')\n",
    "\n",
    "from CanvasHacks.Repositories.quizzes import load_student_work\n",
    "# prev = \"2020-02-08-Unit 1 Peer review-student-work.csv\"\n",
    "latest = \"{}/2020-02-10-Unit 1 Peer review-student-work.csv\".format(unit.review.folder_path)\n",
    "latest = pd.read_csv(latest)\n",
    "latest.rename( { 'id': 'student_id' }, axis=1, inplace=True )\n",
    "latest.set_index('student_id', inplace=True)\n",
    "latest.submitted = pd.to_datetime(latest.submitted)\n",
    "latest = latest[['submitted', 'attempt']]\n",
    "latest['v'] = 'latest'\n",
    "# latest = load_student_work(latest)\n",
    "\n",
    "prev = pd.read_csv('/Users/adam/Desktop/TEMPORARY/unit1peerreview.csv')\n",
    "# prev = load_student_work('/Users/adam/Desktop/TEMPORARY/unit1peerreview.csv')\n",
    "prev.set_index('student_id', inplace=True)\n",
    "prev.submitted = pd.to_datetime(prev.submitted)\n",
    "prev = prev[['submitted', 'attempt']]\n",
    "prev['v'] = 'previous'\n",
    "\n",
    "review_assigns[0].id\n",
    "\n",
    "fs = [prev, latest]\n",
    "fs.sort(key=lambda x: x.submitted.max(), reverse=True)\n",
    "\n",
    "fs[0]\n",
    "\n",
    "import numpy as np\n",
    "assert(prev.submitted.dtype == np.datetime64)\n",
    "\n",
    "prev.submitted = pd.to_datetime(prev.submitted)\n",
    "\n",
    "newsubs = latest[~latest.index.isin(prev.index)]\n",
    "\n",
    "newsubs\n",
    "\n",
    "# Anything which happened after the last check of the data\n",
    "newsubs = latest[latest.submitted > prev.submitted.max()]\n",
    "newsubs\n",
    "\n",
    "prev.submitted.max()\n",
    "\n",
    "# Any second or later attempts that occurred after last \n",
    "# check of data\n",
    "#attempts\n",
    "latest[latest.attempt > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "TEST = True\n",
    "# TEST = False\n",
    "USE_PREVIOUS = False\n",
    "\n",
    "if TEST:\n",
    "    environment.CONFIG.set_test()\n",
    "# environment.CONFIG.set_live()\n",
    "\n",
    "COURSE_ID = environment.CONFIG.course_ids[0]\n",
    "print(\"Working on course: \", COURSE_ID)\n",
    "\n",
    "\n",
    "# UNIT_NUMBER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create assignments (CAN-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CanvasHacks.PeerReviewed.SkaaMaker import UnitDefinitionsLoader, CreationHandlerFactory\n",
    "LOC = '{}/Box Sync/TEACHING/Phil 305 Business ethics/Phil305 S20'.format(environment.ROOT)# placeholder for where the access codes are stored\n",
    "\n",
    "DETAILS_FP = \"{}/Unit details.xlsx\".format(LOC)\n",
    "\n",
    "maker = UnitDefinitionsLoader()\n",
    "maker.load(DETAILS_FP)\n",
    "units = maker.create_units()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in units:\n",
    "    for a in u.components:\n",
    "        maker = CreationHandlerFactory.make(a, environment.CONFIG.course)\n",
    "        print(maker.creation_dict)\n",
    "#         maker.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# test students\n",
    "STUDENT_TEST_ACCT_FP = \"{}/private/test-accounts.xlsx\".format(environment.CONFIG.proj_base)\n",
    "student_users_df = pd.read_excel(STUDENT_TEST_ACCT_FP)\n",
    "tokens = student_users_df.token.dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# for testing\n",
    "fp = \"{}/student-work.csv\".format(initial_work_repo.activity.folder_path )\n",
    "initial_work_repo.data = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "DETAILS_FP = \"{}/Unit details.xlsx\".format(LOC)\n",
    "with pd.ExcelFile(DETAILS_FP) as xls:\n",
    "    unit_details = pd.read_excel(xls, 'details')\n",
    "    start_dates = pd.read_excel(xls, 'startdates')\n",
    "\n",
    "# start_dates.start = start_dates.apply(lambda x: pd.Timestamp(x.start, tz='US/Pacific'), axis=1)\n",
    "start_dates.start = start_dates.apply(lambda x: pd.Timestamp(x.start), axis=1)\n",
    "\n",
    "# Clean up the text\n",
    "unit_details.activity_title = unit_details.activity_title.str.strip()\n",
    "\n",
    "units = []\n",
    "for i, row in start_dates.iterrows():\n",
    "    print(row.unit)\n",
    "    u = Unit([], row.unit)\n",
    "    for activity_type in u.component_types:\n",
    "        activity = activity_type(unit_number=u.unit_number)\n",
    "#         print(activity.title_base)\n",
    "        deets = unit_details[unit_details.activity_title == activity.title_base]\n",
    "#         print(deets)        \n",
    "        setattr(activity, 'unit_number', row.unit)\n",
    "        v = {'unit_number' : row.unit}\n",
    "        d = row.start\n",
    "        \n",
    "#         v['unlock_at'] = d + pd.Timedelta('1 minutes')\n",
    "#         setattr(activity, 'unlock_at', v['unlock_at'])\n",
    "\n",
    "#         d += pd.Timedelta('{} days'.format(deets.due_days_from_start.values[0]))\n",
    "#         v['due_at'] = d - pd.Timedelta('1 minute')\n",
    "#         setattr(activity, 'due_at', v['due_at'])\n",
    "        \n",
    "#         d += pd.Timedelta('{} days'.format(deets.lock_after_due.values[0]))\n",
    "#         v['lock_at'] = d - pd.Timedelta('1 minute')\n",
    "#         setattr(activity, 'lock_at', v['lock_at'])\n",
    "        \n",
    "#         v['points_possible'] = deets.points.values[0]\n",
    "#         setattr(activity, 'points_possible', v['points_possible'])\n",
    "        \n",
    "#         v['description'] = activity.description\n",
    "#         setattr(activity, 'description', v['description'])\n",
    "        \n",
    "        u.components.append(activity)\n",
    "#         u.components.append( activity_type( **v) )\n",
    "    units.append(u)\n",
    "# units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateframe = []\n",
    "for u in units:\n",
    "    for a in u.components:\n",
    "        d = a.__dict__\n",
    "        d['title'] = a.make_title\n",
    "        dateframe.append(d)\n",
    "dateframe = pd.DataFrame(dateframe)\n",
    "dateframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_details[unit_details.activity_title == 'Content assignment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_details.activity_title = unit_details.activity_title.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_details.activity_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get instructions template for the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get questions template where applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe download template quiz and then just replace\n",
    "\n",
    "acts = []\n",
    "d = unit_start_date\n",
    "for activity, days in main:\n",
    "    v = {'unit_number' : UNIT_NUMBER}\n",
    "    v['unlock_at'] = d + pd.Timedelta('1 minutes')\n",
    "    d += pd.Timedelta('{} days'.format(days))\n",
    "    v['due_at'] = d - pd.Timedelta('1 minute')\n",
    "    # todo adjust for grace period\n",
    "    v['lock_at'] = v['due_at']\n",
    "    print(v)\n",
    "    acts.append( activity( **v) )\n",
    "    \n",
    "acts\n",
    "    \n",
    "\n",
    "# todo translate to utc\n",
    "unit_start_date = pd.Timestamp('2020-02-01', tz='US/Pacific')\n",
    "UNIT_NUMBER = 4\n",
    "\n",
    "main = [ (TopicalAssignment, 2),\n",
    "            (InitialWork, 7),\n",
    "            (Review, 2),\n",
    "            (MetaReview, 2)]\n",
    "forum_num_days = [\n",
    "            (DiscussionForum, 7),\n",
    "            (DiscussionReview, 2)]\n",
    "# sum([i[1] for i in num_days])\n",
    "# assert(sum([i[1] for i in num_days]) == 14)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unit dates to df so can give students a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_quiz_results = \"{}/quiz results.csv\".format(test_data_folder)\n",
    "test_quiz_results = pd.read_csv(test_quiz_results)\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "d = test_quiz_results\n",
    "d.name = d.apply(lambda x: fake.name(), axis=1)\n",
    "d.id = d.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "d.sis_id = d.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "d.section_id = d.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "d.section_sis_id = d.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "\n",
    "# d.to_csv(test_quiz_results)\n",
    "\n",
    "d.submission_id = d.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "d.validation_token = d.apply(lambda x: fake.sha1(), axis=1)\n",
    "# submissions_frame.drop(['_requester', 'attributes', 'html_url', 'result_url'], inplace=True, axis=1)\n",
    "d.id = d.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "# d.to_csv(test_quiz_results)\n",
    "d.to_csv(test_combined_frame)\n",
    "d = pd.read_csv(test_combined_frame)\n",
    "# Create test submissions frame\n",
    "\n",
    "def iditer(test_quiz_results):\n",
    "    for i in test_quiz_results.id.tolist():\n",
    "        yield i \n",
    "        \n",
    "fakeids = iditer(test_quiz_results)\n",
    "\n",
    "submissions_frame = pd.read_csv(test_submissions_frame)\n",
    "# cut down to match other frame size\n",
    "submissions_frame = submissions_frame[:len(test_quiz_results)]\n",
    "\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "fake = Faker()\n",
    "\n",
    "# quiz_id = random.randint( 11111, 99999 )\n",
    "quiz_id = prev_quiz\n",
    "submissions_frame.quiz_id = quiz_id\n",
    "submissions_frame.course_id = test_course_id\n",
    "submissions_frame.user_id = submissions_frame.apply(lambda x: next(fakeids), axis=1)\n",
    "submissions_frame.submission_id = submissions_frame.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "submissions_frame.validation_token = submissions_frame.apply(lambda x: fake.sha1(), axis=1)\n",
    "# submissions_frame.drop(['_requester', 'attributes', 'html_url', 'result_url'], inplace=True, axis=1)\n",
    "submissions_frame.id = submissions_frame.apply(lambda x: fake.ean(length=8), axis=1)\n",
    "\n",
    "submissions_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_reports(course_id, quiz_id):\n",
    "    \"\"\"\n",
    "     GET /api/v1/courses/:course_id/quizzes/:quiz_id/reports \n",
    "     \"\"\"\n",
    "    url = make_url(course_id, 'quizzes')\n",
    "    url = \"{}/{}/reports\".format(url, quiz_id)\n",
    "    print(\"getting \", url)\n",
    "    response = requests.get(url, headers=make_request_header())\n",
    "    return response\n",
    "#     return send_get_request(url)\n",
    "\n",
    "\n",
    "\n",
    "# r = get_all_reports(course_id, main_quiz)\n",
    "r = get_all_reports(course_id, review_quiz)\n",
    "# r = get_all_reports(course_id, meta_review)\n",
    "# r = get_all_reports(prev_course_id, prev_quiz)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def request_report_creation(course_id, quiz_id):\n",
    "    \"\"\"\n",
    "    ALWAYS RETURNS ERRORS\n",
    "    First, we need to tell canvas to make the report \n",
    "    by hitting:  POST /api/v1/courses/:course_id/quizzes/:quiz_id/reports\n",
    "    In theory this could return the report url\n",
    "    \"\"\"\n",
    "    url = make_url(course_id, 'quizzes')\n",
    "    url = \"{}/{}/reports\".format(url, quiz_id)\n",
    "\n",
    "    data = {\"quiz_report\" : [{\"report_type\":\"student_analysis\"}],\n",
    "            \"include\":[\"progress\",\"file\"]\n",
    "           }\n",
    "    headers = { 'Authorization': 'Bearer {}'.format( environment.CONFIG.canvas_token ),\n",
    "              'Content-Type' : 'application/json'\n",
    "              }\n",
    "    data = json.dumps(data)\n",
    "    result = requests.request(\"POST\", url, data=data, headers=headers)\n",
    "#     result = send_post_request(url, data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_report_urls(course_id, quiz_id):\n",
    "    \"\"\"Now that the report, hopefully, exists, we can try to get the \n",
    "    urls to it\n",
    "    \"\"\"\n",
    "    url = make_url(course_id, 'quizzes')\n",
    "    url = \"{}/{}/reports\".format(url, quiz_id)\n",
    "    \n",
    "    print(\"Course {} Quiz {} : Requesting {}\".format(course_id, quiz_id, url))\n",
    "\n",
    "    data = {\n",
    "#         \"quiz_report\":[{\"report_type\":\"student_analysis\",\"includes_all_versions\":False}],\n",
    "            \"include\":[\"progress\",\"file\"]\n",
    "           }\n",
    "    result = send_get_request(url, data)\n",
    "    print(result)\n",
    "    \n",
    "    if len(result) == 1:\n",
    "        result = [result]\n",
    "\n",
    "    # Get the right type of report\n",
    "    reports = [r for r in result if r['report_type'] == 'student_analysis']\n",
    "    print(\"{} report urls received\".format(len(reports)))\n",
    "    urls = [r['url'] for r in reports]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = request_report_creation(course_id, main_quiz)\n",
    "r = request_report_creation(course_id, review_quiz)\n",
    "# r = request_report_creation(course_id, meta_review)\n",
    "# r = request_report_creation(prev_course_id, prev_quiz)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the url to the report for the quiz\n",
    "\n",
    "# report_urls = get_report_urls(course_id, main_quiz) #specified resource dn exist\n",
    "report_urls = get_report_urls(course_id, review_quiz) #works\n",
    "# report_urls = get_report_urls(course_id, meta_review) #works; also returns item_analysis report\n",
    "# report_urls = get_report_urls(prev_course_id, prev_quiz) #works; also returns item_analysis report\n",
    "\n",
    "report_url = report_urls[0]\n",
    "\n",
    "print(\"Link to report : \", report_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def utc_string_to_local_dt(utc_string):\n",
    "    \"\"\"'2020-02-07T07:59:59Z'\n",
    "    returns Timestamp('2020-02-06 23:59:59-0800', tz='US/Pacific')\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(utc_string).tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# def report_url_gen(starting_report_url):\n",
    "#     \"\"\"\n",
    "#     Returns the original url followed by copies with the report id incremented by 1\n",
    "#     starting_report_url: 'https://canvas.csun.edu/api/v1/courses/85210/quizzes/165098/reports/92640'\n",
    "#     \"\"\"\n",
    "#     r = starting_report_url.split('/')\n",
    "#     url_stub =\"/\".join(r[:-1])\n",
    "#     rid = int(r[-1:][0])\n",
    "\n",
    "#     while True:\n",
    "#         yield \"{}/{}?include[]=progress&include[]=file\".format(url_stub, rid)\n",
    "#         rid += 1\n",
    "    \n",
    "\n",
    "\n",
    "# def parse_download_url(response):\n",
    "#     \"\"\"Takess a requests.response from get_report_download_url\n",
    "#     and tries to parse out the file download url.\n",
    "#     todo: set this up to poll for progress \n",
    "#     \"\"\"\n",
    "#     c = json.loads(response.content)\n",
    "\n",
    "#     try:\n",
    "#         download_url = c['file']['url']\n",
    "#         print(download_url)\n",
    "#         return download_url\n",
    "    \n",
    "#     except KeyError:\n",
    "#         # todo Poll the progress url until have download url\n",
    "# #         progress_url = c['progress_url']\n",
    "#         print(\"Error: No download url provided \\n\", c)\n",
    "\n",
    "\n",
    "# def get_report_download_url(report_url):\n",
    "#     \"\"\"Uses the report url to get the url from which the csv file can be downloaded\"\"\"\n",
    "# #     data = {\"include\": [\"progress\", \"file\"]}\n",
    "# #     data = {\"include\": \"file\"}\n",
    "#     url = \"{}?include[]=progress&include[]=file\".format(report_url)\n",
    "#     data = {}\n",
    "\n",
    "#     try:\n",
    "#         # Make the request\n",
    "#         # We can't use the usual function since we don't want response.json()\n",
    "#         response = requests.get(report_url, headers=make_request_header())\n",
    "\n",
    "#         print('response was ', response.status_code)\n",
    "#         response.raise_for_status()\n",
    "#         # todo or put check the progress url until workflow_state=\"completed\"\n",
    "# #         if response.progress.workflow_state != 'completed':\n",
    "            \n",
    "#         # Continuing on since was successful\n",
    "#         return parse_download_url(response)\n",
    "        \n",
    "#     except HTTPError as http_err:\n",
    "#         print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "\n",
    "#     except Exception as err:\n",
    "#         print(f'Other error occurred: {err}')  # Python 3.6\n",
    "\n",
    "\n",
    "\n",
    "# def download_report(download_url, save_file_path=None):\n",
    "#     \"\"\"Once we have the url to download the csv file, this\n",
    "#     will handle the download and load the data into a pandas\n",
    "#     dataframe\n",
    "#     \"\"\"\n",
    "#     # Request the results file for assignment\n",
    "#     # Open it with pandas\n",
    "#     frame = pd.read_csv(download_url)\n",
    "\n",
    "#     if save_file_path:\n",
    "#         # Temporary location to store the downloaded file\n",
    "#         frame.to_csv(save_file_path)\n",
    "\n",
    "#     return frame\n",
    "\n",
    "\n",
    "# def retrieve_quiz_data(quiz, rest_timeout=60, max_id_attempts=20):\n",
    "#     \"\"\"Returns a dataframe of the student report\n",
    "#     ACTUALLY WORKING VERSION!\n",
    "#     WILL ONLY WORK IF THE GENERATE REPORT BUTTON HAS BEEN MANUALLY CLICKED FIRST\n",
    "#     Returns:\n",
    "#         DataFrame\n",
    "        \n",
    "#     \"\"\"\n",
    "#     # This should request that the reports be generated\n",
    "#     reports = quiz.get_all_quiz_reports()\n",
    "    \n",
    "#     print(\"Resting for {} seconds while waiting for canvas to generate report\".format(rest_timeout))\n",
    "#     time.sleep(rest_timeout)\n",
    "    \n",
    "#     # The first report should be the student_analysis report. However, \n",
    "#     # for some reason, canvas will not return the file info with the \n",
    "#     # report we just created. However, it seems that the creation process \n",
    "#     # also created a report with a different id, which will contain the file info\n",
    "#     # At least in testing, this report id was 2 more than the student report\n",
    "#     # (the item_analysis report id was the student_analysis id + 1)\n",
    "#     # But will use a generator to cover more possibilities\n",
    "#     url_gen = report_url_gen(reports[0].url)\n",
    "    \n",
    "#     for _ in range(0, max_id_attempts):\n",
    "#         # first value out of generator will be the original url\n",
    "#         url = next(url_gen)\n",
    "#         print(\"trying: \", url)\n",
    "#         # We request the hidden report object which will have the url for downloading\n",
    "#         # and parse out the url\n",
    "#         download_url = get_report_download_url(url)\n",
    "#         # We load the url and parse into a dataframe\n",
    "#         if download_url:\n",
    "#             return download_report(download_url)\n",
    "\n",
    "\n",
    "# def save_downloaded_report(activity, frame):\n",
    "#     \"\"\"If we've downloaded the report programmatically, this \n",
    "#     saves it to the expected location\n",
    "#     \"\"\"\n",
    "#     # save to file\n",
    "#     create_folder(activity.folder_path)\n",
    "#     try:\n",
    "#         # if there's a particular section\n",
    "#         fp = \"{}/{}-student-work.csv\".format(activity.folder_path, SECTION )\n",
    "#     except NameError:\n",
    "#         fp = \"{}/{}-{}-student-work.csv\".format(activity.folder_path, getDateForMakingFileName(), activity.safe_name )\n",
    "\n",
    "#     try:\n",
    "#         frame.to_csv(fp)\n",
    "#     except Exception as e:\n",
    "#         print(\"Error saving student work to file \", e)\n",
    "\n",
    "from CanvasHacks.QuizReportFileTools import retrieve_quiz_data, save_downloaded_report\n",
    "\n",
    "def make_quiz_repo( course, activity, save=True):\n",
    "    \"\"\"Gets all student work data for the activity that's part of the assignment\n",
    "    loads it into a QuizRepository and \n",
    "    returns the repository\n",
    "    \"\"\"\n",
    "    # Get quiz submission objects\n",
    "    if isinstance(activity, Review):\n",
    "        repo = ReviewRepository(activity, course)\n",
    "    else:\n",
    "        repo = QuizRepository(activity, course)\n",
    "    \n",
    "    # The InitialWork object will have the assignment id set as its id\n",
    "    # if the type of assignment is a quiz, we will need the quiz id\n",
    "    # That matters because it seems we can't use activity.id for topical assignment\n",
    "    # because the object returned will be a quiz report \n",
    "#     assign = course.get_assignment(activity.id)\n",
    "#     if assign.is_quiz_assignment:\n",
    "#         quiz_id = assign.quiz_id\n",
    "#         quiz = course.get_quiz(quiz_id)\n",
    "#     quiz = course.get_quiz(activity.quiz_id)\n",
    "\n",
    "    # Download submissions\n",
    "    subRepo = QuizSubmissionRepository(repo.quiz)\n",
    "#     submissions = quiz.get_submissions()    \n",
    "    \n",
    "    # the canvasapi has returned a bunch of objects\n",
    "    # so we convert to dicts to make easier to create dataframe\n",
    "#     submissions = [l.__dict__ for l in list(submissions)]\n",
    "\n",
    "    # Download student work\n",
    "    # This will work if the 'Create Report' button has been manually clicked\n",
    "    student_work_frame = retrieve_quiz_data(repo.quiz)\n",
    "    \n",
    "    if save:\n",
    "        save_downloaded_report(activity, student_work_frame)\n",
    "    \n",
    "#     if isinstance(activity, Review):\n",
    "#         repo = ReviewRepository(activity, course)\n",
    "#     else:\n",
    "#         repo = QuizRepository(activity)\n",
    "    \n",
    "    # Doing the combination with submissions after saved to avoid \n",
    "    # mismatches of new and old data\n",
    "    repo._process(student_work_frame, subRepo.frame)\n",
    "    \n",
    "    return repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_get_request(url.format(main_quiz), { 'include': 'submission'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "download_url = parse_download_url(response)\n",
    "\n",
    "response_content_for_testing = b'{\"id\":91464,\"report_type\":\"student_analysis\",\"readable_type\":\"Student Analysis\",\"includes_all_versions\":false,\"includes_sis_ids\":true,\"generatable\":true,\"anonymous\":false,\"url\":\"https://canvas.csun.edu/api/v1/courses/85210/quizzes/165820/reports/91464\",\"progress_url\":\"https://canvas.csun.edu/api/v1/progress/335965\",\"created_at\":\"2019-12-27T01:45:29Z\",\"updated_at\":\"2019-12-27T01:55:54Z\",\"file\":{\"id\":7868563,\"uuid\":\"VZNHEVLsfBZt2IJYCvkBdmM2x5LfqIBVjXoRmDVo\",\"folder_id\":null,\"display_name\":\"Quiz1-review Survey Student Analysis Report.csv\",\"filename\":\"quiz_student_analysis_report.csv\",\"upload_status\":\"success\",\"content-type\":\"text/csv\",\"url\":\"https://canvas.csun.edu/files/7868563/download?download_frd=1\\\\u0026verifier=VZNHEVLsfBZt2IJYCvkBdmM2x5LfqIBVjXoRmDVo\",\"size\":132,\"created_at\":\"2019-12-27T01:55:54Z\",\"updated_at\":\"2019-12-27T01:55:54Z\",\"unlock_at\":null,\"locked\":false,\"hidden\":false,\"lock_at\":null,\"hidden_for_user\":false,\"thumbnail_url\":null,\"modified_at\":\"2019-12-27T01:55:54Z\",\"mime_class\":\"file\",\"media_entry_id\":null,\"locked_for_user\":false},\"quiz_id\":165820}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "#     'quiz_report': [{\n",
    "#         'report_type': 'student_analysis',\n",
    "#     }],\n",
    "    \"include\": [\"progress\", \"file\"]\n",
    "}\n",
    "\n",
    "# This report url works properly!!!!!!!!!!!!!!!!\n",
    "report_url = 'https://canvas.csun.edu/api/v1/courses/85210/quizzes/165820/reports/91464'\n",
    "# Make the request \n",
    "# We can't use the usual function since we don't want response.json()\n",
    "head = { 'Authorization': 'Bearer {}'.format( environment.CONFIG.canvas_token ) }\n",
    "response = requests.get( report_url, headers=make_request_header(), json=data )\n",
    "\n",
    "print('response was ', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# report_url = 'https://canvas.csun.edu/api/v1/courses/85210/quizzes/165820/reports/91464'\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example of what looking for__\n",
    "\n",
    "send_get_request('https://canvas.csun.edu/api/v1/courses/85210/quizzes/165820/reports/91464', {'include' : 'file'})\n",
    "\n",
    "{'id': 91464,\n",
    " 'report_type': 'student_analysis',\n",
    " 'readable_type': 'Student Analysis',\n",
    " 'includes_all_versions': False,\n",
    " 'includes_sis_ids': True,\n",
    " 'generatable': True,\n",
    " 'anonymous': False,\n",
    " 'url': 'https://canvas.csun.edu/api/v1/courses/85210/quizzes/165820/reports/91464',\n",
    " 'progress_url': 'https://canvas.csun.edu/api/v1/progress/335965',\n",
    " 'created_at': '2019-12-27T01:45:29Z',\n",
    " 'updated_at': '2019-12-27T01:55:54Z',\n",
    " 'file': {'id': 7868563,\n",
    "  'uuid': 'VZNHEVLsfBZt2IJYCvkBdmM2x5LfqIBVjXoRmDVo',\n",
    "  'folder_id': None,\n",
    "  'display_name': 'Quiz1-review Survey Student Analysis Report.csv',\n",
    "  'filename': 'quiz_student_analysis_report.csv',\n",
    "  'upload_status': 'success',\n",
    "  'content-type': 'text/csv',\n",
    "  'url': 'https://canvas.csun.edu/files/7868563/download?download_frd=1&verifier=VZNHEVLsfBZt2IJYCvkBdmM2x5LfqIBVjXoRmDVo',\n",
    "  'size': 132,\n",
    "  'created_at': '2019-12-27T01:55:54Z',\n",
    "  'updated_at': '2019-12-27T01:55:54Z',\n",
    "  'unlock_at': None,\n",
    "  'locked': False,\n",
    "  'hidden': False,\n",
    "  'lock_at': None,\n",
    "  'hidden_for_user': False,\n",
    "  'thumbnail_url': None,\n",
    "  'modified_at': '2019-12-27T01:55:54Z',\n",
    "  'mime_class': 'file',\n",
    "  'media_entry_id': None,\n",
    "  'locked_for_user': False},\n",
    " 'quiz_id': 165820}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while(1);{\"id\":91464,\"report_type\":\"student_analysis\",\"readable_type\":\"Student Analysis\",\"includes_all_versions\":false,\"includes_sis_ids\":true,\"generatable\":true,\"anonymous\":false,\"url\":\"https://canvas.csun.edu/api/v1/courses/85210/quizzes/165820/reports/91464\",\"progress_url\":\"https://canvas.csun.edu/api/v1/progress/335965\",\"created_at\":\"2019-12-27T01:45:29Z\",\"updated_at\":\"2019-12-27T01:55:54Z\",\"file\":{\"id\":7868563,\"uuid\":\"VZNHEVLsfBZt2IJYCvkBdmM2x5LfqIBVjXoRmDVo\",\"folder_id\":null,\"display_name\":\"Quiz1-review Survey Student Analysis Report.csv\",\"filename\":\"quiz_student_analysis_report.csv\",\"upload_status\":\"success\",\"content-type\":\"text/csv\",\"url\":\"https://canvas.csun.edu/files/7868563/download?download_frd=1\\u0026verifier=VZNHEVLsfBZt2IJYCvkBdmM2x5LfqIBVjXoRmDVo\",\"size\":132,\"created_at\":\"2019-12-27T01:55:54Z\",\"updated_at\":\"2019-12-27T01:55:54Z\",\"unlock_at\":null,\"locked\":false,\"hidden\":false,\"lock_at\":null,\"hidden_for_user\":false,\"thumbnail_url\":null,\"modified_at\":\"2019-12-27T01:55:54Z\",\"mime_class\":\"file\",\"media_entry_id\":null,\"locked_for_user\":false},\"quiz_id\":165820}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'include' : ['submission_comments', 'user']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def download_submissions(course_id, quiz_id):\n",
    "    \"\"\"This isn't really needed\"\"\"\n",
    "    course = canvas.get_course(course_id)\n",
    "    quiz = course.get_quiz(quiz_id)\n",
    "    submissions = quiz.get_submissions()\n",
    "    submissions_frame = pd.DataFrame([l.__dict__ for l in list(submissions)])\n",
    "    return submissions_frame\n",
    "\n",
    "if TEST:\n",
    "    if USE_PREVIOUS:\n",
    "        # test -- using previous course\n",
    "        submissions_frame = download_submissions(prev_course_id, prev_quiz)\n",
    "    else:\n",
    "        # test\n",
    "        submissions_frame = pd.read_csv(test_submissions_frame)\n",
    "# submissions_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "\n",
    "    if USE_PREVIOUS:\n",
    "    #     or, if testing on past class\n",
    "        all_quizzes = {q['id'] : q for q in get_all_course_quizzes(prev_course_id)}\n",
    "        iq = all_quizzes.get(prev_quiz)\n",
    "        initial = InitialWork(**iq)\n",
    "        initial_work_repo = make_quiz_repo(initial)\n",
    "        initial_work_repo \n",
    "        \n",
    "    else:\n",
    "        # Testing \n",
    "        initial_work_repo = QuizRepository(unit.initial_work)\n",
    "        initial_work_repo.data = pd.read_csv(test_combined_frame)\n",
    "        initial_work_repo._cleanup_data()\n",
    "#         initial_work_repo._process(student_work_frame, submissions_frame)\n",
    "\n",
    "else:\n",
    "    # real\n",
    "    initial_work_repo = make_quiz_repo(unit.initial_work)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "if not TEST:\n",
    "    # Real version\n",
    "    course = canvas.get_course(COURSE_ID)\n",
    "    unit = Unit(course, UNIT_NUMBER)\n",
    "\n",
    "\n",
    "#     # Download canvas data\n",
    "#     all_quizzes = {q['id'] : q for q in get_all_course_quizzes(COURSE_ID)}\n",
    "\n",
    "#     # Use canvas data to make objects defining each part of the assignment\n",
    "#     iq = all_quizzes.get(part_ids[0])\n",
    "#     initial = InitialWork(**iq)\n",
    "\n",
    "#     rq = all_quizzes.get(part_ids[1])\n",
    "#     review = Review(**rq)\n",
    "#     review.access_code = codeRepo.review_code\n",
    "\n",
    "#     mq = all_quizzes.get(part_ids[2])\n",
    "#     meta = MetaReview(**mq)\n",
    "#     meta.access_code = codeRepo.metareview_code\n",
    "\n",
    "#     assignment = Assignment( initial, review, meta )\n",
    "\n",
    "else:\n",
    "    # Test version \n",
    "\n",
    "    from tests.factories.PeerReviewedFactories import *\n",
    "    from tests.factories.ModelFactories import *\n",
    "    test_data = test_data_factory()\n",
    "\n",
    "    initial = InitialWork( **test_data[ 'initial' ] )\n",
    "    initial.id = main_quiz\n",
    "    # todo: add the quiz data into initial\n",
    "    initial.question_columns = ['c1', 'c2']\n",
    "\n",
    "    # todo should also be setting name, access code and link\n",
    "    review = Review( **test_data[ 'review' ] )\n",
    "    review.id = review_quiz\n",
    "    review.access_code = codeRepo.review_code\n",
    "\n",
    "    meta = MetaReview( **test_data[ 'metareview' ] )\n",
    "    meta.id = meta_review\n",
    "    meta.access_code = codeRepo.metareview_code\n",
    "    \n",
    "    unit = Unit({}, UNIT_NUMBER)\n",
    "    unit.components = [ initial, review, meta]\n",
    "#     unit = Assignment( initial, review, meta )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "canvashacks",
   "language": "python",
   "name": "canvashacks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
