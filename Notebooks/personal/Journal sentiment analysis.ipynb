{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/Dropbox/CanvasHacks\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "#http://blog.rtwilson.com/how-to-get-nice-vector-graphics-in-your-exported-pdf-ipython-notebooks/\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette('plasma'))\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Latex\n",
    "\n",
    "from CanvasHacks import environment\n",
    "from CanvasHacks.Api.RequestTools import get_all_course_assignments, get_assignments_with_submissions\n",
    "from CanvasHacks.Definitions.journal import Journal\n",
    "\n",
    "# Import the Canvas class\n",
    "from canvasapi import Canvas\n",
    "\n",
    "# files\n",
    "from CanvasHacks.Files.FileTools import makeDataFileIterator, makeDataFileList\n",
    "\n",
    "# repos\n",
    "from CanvasHacks.Repositories.students import StudentRepository\n",
    "from CanvasHacks.Repositories.submissions import SubmissionRepository\n",
    "# text tools\n",
    "from CanvasHacks.Text.process import WordbagMaker, TokenFiltrationMixin\n",
    "from CanvasHacks.Text.cleaners import TextCleaner\n",
    "from CanvasHacks.Text.stats import WordFreq\n",
    "import json\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# This will be used to cut off the historical data\n",
    "CURRENT_WEEK = 13\n",
    "\n",
    "CURRENT_TERM = 'S20'\n",
    "\n",
    "JOURNALS_FOLDER = \"{}/journals\".format(environment.LOG_FOLDER)\n",
    "CONTENT_FOLDER =\"{}/content\".format(JOURNALS_FOLDER)  \n",
    "BAG_FOLDER = \"{}/bags\".format(JOURNALS_FOLDER) \n",
    "\n",
    "course_ids = environment.CONFIG.course_ids\n",
    "class_ids = {\n",
    "    'F19': [62657, 67473, 62660],\n",
    "    'F18': [41179, 41180, 41181],\n",
    "    'S19': [67531],\n",
    "    'S20': environment.CONFIG.course_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_content_filepath(term, week_num, course_id=None, folder=CONTENT_FOLDER, **kwargs):\n",
    "    return \"{}/{}-{}-week{}-content.json\".format(folder, term, course_id, week_num)\n",
    "\n",
    "def make_bag_filepath(term, week_num, course_id=None, folder=BAG_FOLDER, **kwargs):\n",
    "    return \"{}/{}-{}-week{}-bag.json\".format(folder, term, course_id, week_num)\n",
    "\n",
    "\n",
    "def make_week_iterator(start=7, stop=CURRENT_WEEK):\n",
    "    for w in range(start, stop + 1):\n",
    "        yield w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acquire, clean, and store text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_all_journal_assigns_for_class(course_id, term):\n",
    "    assignments = [ ]\n",
    "    # Get list of all assignments for the courses\n",
    "    assignments += get_all_course_assignments( course_id )\n",
    "    assignments = [ (a[ 'id' ], a[ 'name' ].strip()) for a in assignments ]\n",
    "\n",
    "    # If we we're passed an activity_inviting_to_complete, filter the assignments\n",
    "    assignments = [a for a in assignments if Journal.is_activity_type(a[1])]\n",
    "\n",
    "    assignments = [{ 'term': term, \n",
    "                    'course_id': course_id, \n",
    "                    'id': a[0], \n",
    "                    'week_num' : int(a[1].split(' ')[-1][ : -1])\n",
    "                   } for a in assignments]\n",
    "    return assignments\n",
    "\n",
    "\n",
    "\n",
    "def store_course_journals(course_id, term, start_week=None):\n",
    "    \"\"\"\n",
    "    Downloads and saves journals\n",
    "    \"\"\"\n",
    "    # may want to run later with True so can look at uses of I/me for depression\n",
    "    course = environment.CONFIG.canvas.get_course(course_id)\n",
    "    journals = get_all_journal_assigns_for_class(course_id, term)\n",
    "    cleaner = TextCleaner()\n",
    "    for j in journals:\n",
    "        # doing first so won't waste time\n",
    "        fp = make_content_filepath(**j, )\n",
    "        print(fp)\n",
    "        if start_week is not None and j['week_num'] < start_week:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # Download submissions\n",
    "            assignment = course.get_assignment(j['id'])\n",
    "            print(\"Downloading {} {}\".format(j['term'], j['week_num']))\n",
    "            subRepo = SubmissionRepository(assignment)\n",
    "            print(\"{} journals downloaded\".format(len(subRepo.data)))\n",
    "\n",
    "            j['content'] =[{'sid': d.user_id, 'body': cleaner.clean(d.body)} for d in subRepo.data if d.body is not None]\n",
    "\n",
    "            with open(fp, 'w') as f:\n",
    "                json.dump(j, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Get enrollments for courses\n",
    "enrollments = {}\n",
    "\n",
    "for term, ids in class_ids.items():\n",
    "    cnt = 0\n",
    "    for cid in ids:\n",
    "        course = environment.CONFIG.canvas.get_course(cid)\n",
    "        students = [u for u in course.get_users()]\n",
    "        cnt += len(students) - 1\n",
    "    enrollments[term] = cnt\n",
    "enrollments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and store journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "courses_to_get = [  ]\n",
    "\n",
    "# for cid in courses_to_get:\n",
    "#     store_course_journals(cid, 'S19')\n",
    "\n",
    "\n",
    "for cid in environment.CONFIG.course_ids:\n",
    "    store_course_journals(cid, 'S20', start_week=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make and store wordbags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def process_journal_entries(journal_entries, existing=[]):\n",
    "    \"\"\"\n",
    "    Tokenizes and lightly filters a list of journal entries\n",
    "    before saving to json\n",
    "    \"\"\"\n",
    "    \n",
    "    fp = make_bag_filepath(**journal_entries)\n",
    "    print(fp)\n",
    "    \n",
    "    if fp not in existing:\n",
    "        bagmaker = WordbagMaker(keep_stopwords=True)\n",
    "        \n",
    "        if len(journal_entries['content']) > 0: \n",
    "            for entry in journal_entries['content']:\n",
    "#                 print(entry)\n",
    "                if len(entry['body']) > 0:\n",
    "                    entry['bag'] = bagmaker.process(entry['body'])\n",
    "\n",
    "        with open(fp, 'w') as f:\n",
    "            json.dump(journal_entries, f)\n",
    "        \n",
    "    return journal_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "fiter = makeDataFileIterator(CONTENT_FOLDER)\n",
    "existing = makeDataFileList(BAG_FOLDER)\n",
    "\n",
    "while True:\n",
    "    with open(next(fiter), 'r') as f:\n",
    "        print(\"Processing \", f.name.split('/')[-1:])\n",
    "        entries = json.load(f)\n",
    "        process_journal_entries(entries, existing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A journal assignment which was done by one class on one week in one term\n",
    "# Thus on one week in one term there may be multiple CourseJournals\n",
    "CourseJournal = namedtuple('Week', ['term', \n",
    "                            # The canvas course id for the journal\n",
    "                            # 'course_id', \n",
    "                             \n",
    "                             # The canvas assignment id\n",
    "                             'id', \n",
    "                             \n",
    "                             'week_num', \n",
    "                             \n",
    "                             # A list of dictionaries containing student\n",
    "                             # journal entries.\n",
    "                             # Each dictionary has the keys:\n",
    "                             #    sid\n",
    "                             #    body: The sanitized text of the journal\n",
    "                             #    bag: List of word tokens, including stopwords\n",
    "                             'content'])\n",
    "\n",
    "class JournalAssignment(TokenFiltrationMixin):\n",
    "    \"\"\"\n",
    "    A journal assignment which was done by one class on one week in one term\n",
    "    Thus on one week in one term there may be multiple CourseJournals\n",
    "    This holds the data and handles most calculations via properties\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        self._fix_errors()\n",
    "#         self.token_filter = TokenFiltrationMixin()\n",
    "    \n",
    "    @property\n",
    "    def bags(self):\n",
    "        \"\"\"Returns a list of all the student wordbags\"\"\"\n",
    "        return [ s['bag'] for s in self.content]\n",
    "    \n",
    "    @property\n",
    "    def combo_bag(self):\n",
    "        \"\"\"A wordbag comprising every word submitted by students \n",
    "        \n",
    "        Removes punctuation which didn't get filtered when bag created\n",
    "        \"\"\"\n",
    "        b = []\n",
    "        [b.extend(l) for l in self.bags]\n",
    "        return b\n",
    "    \n",
    "    @property\n",
    "    def no_stops_bag(self):\n",
    "        \"\"\"A wordbag comprising every word submitted by students sans stopwords\"\"\"\n",
    "        return [self.clean_punctuation(w) for w in self.combo_bag if self.keep(w, keep_stopwords=False)] # not in self.to_remove]\n",
    "    \n",
    "    @property\n",
    "    def total_sentiment(self):\n",
    "        return self.calc_sentiment(self.combo_bag)\n",
    "    \n",
    "    @property\n",
    "    def average_sentiment(self):\n",
    "        return self.total_sentiment / self.word_count\n",
    "\n",
    "    @property\n",
    "    def word_count(self):\n",
    "        return len(self.combo_bag)\n",
    "    \n",
    "    @property\n",
    "    def num_empty(self):\n",
    "        return len([b for b in self.bags if len(b) == 0])\n",
    "        \n",
    "    @property\n",
    "    def num_students(self):\n",
    "        return len(self.content)\n",
    "    \n",
    "    @property\n",
    "    def student_sentiments(self):\n",
    "        return [self.calc_sentiment(bag) for bag in self.bags]\n",
    "    \n",
    "    @property\n",
    "    def student_avg_sentiments(self):\n",
    "        return [self.calc_sentiment(bag) / len(bag) for bag in self.bags if len(bag) > 0]\n",
    "    \n",
    "    \n",
    "    def calc_sentiment(self, bag):\n",
    "        \"\"\"\n",
    "        Calculates a total sentiment score of items in the bag\n",
    "        \"\"\"\n",
    "        txt = ' '.join(bag)\n",
    "        return afinn.score(txt)\n",
    "\n",
    "    def _fix_errors(self):\n",
    "        for c in self.content:\n",
    "            # Not sure why but one entry from f19 has no bag.\n",
    "            # maybe was the instructor or test student\n",
    "            if 'bag' not in c.keys():\n",
    "                c['bag'] = []\n",
    "                \n",
    "        \n",
    "\n",
    "class TermWeekStore(TokenFiltrationMixin):\n",
    "    \"\"\"Represents a particular week in a particular term\n",
    "    Handles combining multiple classes into one data store\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, term, week, all_journals):\n",
    "        self.term = term\n",
    "        self.week = week\n",
    "        self.journals = [ d for d in all_journals if d.term == self.term and d.week_num == self.week]\n",
    "#             print(len(week_journals))\n",
    "#         self.token_filter = TokenFiltrationMixin()\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def bags(self):\n",
    "        \"\"\"Returns a list of all the student wordbags\"\"\"\n",
    "        b = []\n",
    "        [b.extend(g.bags) for g in self.journals ]\n",
    "        return b\n",
    "#         return [ b for b in j.bags for j in self.journals]\n",
    "    \n",
    "    @property\n",
    "    def combo_bag(self):\n",
    "        \"\"\"A wordbag comprising every word submitted by students \"\"\"\n",
    "        b = []\n",
    "        [b.extend(l) for l in self.bags]\n",
    "        return b\n",
    "    \n",
    "    @property\n",
    "    def no_stops_bag(self):\n",
    "        \"\"\"A wordbag comprising every word submitted by students sans stopwords\"\"\"\n",
    "        return [self.clean_punctuation(w) for w in self.combo_bag if self.keep(w, keep_stopwords=False)] # not in self.to_remove]\n",
    "    \n",
    "    @property\n",
    "    def total_sentiment(self):\n",
    "        return self.calc_sentiment(self.combo_bag)\n",
    "    \n",
    "    @property\n",
    "    def average_sentiment(self):\n",
    "        return self.total_sentiment / self.word_count\n",
    "\n",
    "    @property\n",
    "    def word_count(self):\n",
    "        return len(self.combo_bag)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def week_num(self):\n",
    "        return self.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "fiter = makeDataFileIterator(BAG_FOLDER)\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        with open(next(fiter), 'r') as f:\n",
    "            d = json.load(f)\n",
    "            o = JournalAssignment(**d)\n",
    "            data.append(o)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"Loaded {} files\".format(len(data)))\n",
    "\n",
    "terms = list(set([e.term for e in data]))\n",
    "weeks = list(set([e.week_num for e in data]))\n",
    "\n",
    "week_stores = []\n",
    "for t in terms:\n",
    "    for w in weeks:\n",
    "        week_stores.append(TermWeekStore(t, w, data))\n",
    "\n",
    "len(week_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiter = makeDataFileIterator(BAG_FOLDER)\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        with open(next(fiter), 'r') as f:\n",
    "            d = json.load(f)\n",
    "            o = JournalAssignment(**d)\n",
    "            data.append(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistics \n",
    "\n",
    "store = { t : { w : [] for w in weeks } for t in terms }\n",
    "# store\n",
    "\n",
    "Stat = namedtuple('Stat', [\n",
    "    'term', 'week', 'total_sentiment', \n",
    "    \n",
    "    # Total number of words submitted by all students for that week\n",
    "    'word_count', \n",
    "    \n",
    "    # The number of students who didn't submit the assignment\n",
    "    'num_empty', \n",
    "    \n",
    "    # The number of students enrolled for the week. \n",
    "    # Be careful using this because varies across terms\n",
    "    'num_students'\n",
    "])\n",
    "\n",
    "NoStopStat = namedtuple('NoStopStat', Stat._fields)\n",
    "\n",
    "stats_data = []\n",
    "\n",
    "for t in terms:\n",
    "    for w in weeks:\n",
    "        try:\n",
    "            week_journals = [ d for d in data if d.term == t and d.week_num == w]\n",
    "#             print(len(week_journals))\n",
    "            \n",
    "            if len(week_journals) > 0:\n",
    "                s = Stat(term=t,\n",
    "                         week=w,\n",
    "                         total_sentiment=sum([j.total_sentiment for j in week_journals]),\n",
    "                         word_count=sum([j.word_count for j in week_journals]),\n",
    "                         num_empty=sum([j.num_empty for j in week_journals]),\n",
    "                         num_students=sum([j.num_students for j in week_journals]))\n",
    "                stats_data.append(s)\n",
    "            \n",
    "        except IndexError:\n",
    "            print('index error ', t, w)\n",
    "\n",
    "stats_data = pd.DataFrame(stats_data)\n",
    "\n",
    "# Calculate what percentage of enrolled studens turned the assignment in\n",
    "# Note that we are not going to subtract out the num_empty because those may \n",
    "# be students who turned in a file whose contents we were unable to extract.\n",
    "def calc_pct_comp(row, enrollments=enrollments):\n",
    "    \"\"\"Returns the percentage of enrolled students who have turned it in\"\"\"\n",
    "    return row.num_students / enrollments.get(row.term)\n",
    "\n",
    "stats_data['pct_completion'] = stats_data.apply(lambda x: calc_pct_comp(x), axis=1)\n",
    "\n",
    "# make working copy\n",
    "stats = stats_data[stats_data.week <= CURRENT_WEEK].copy(deep=True)\n",
    "stats = stats[stats.word_count > 0]\n",
    "\n",
    "# Calculate the total sentiment divided by word count, so we can compare\n",
    "stats['sentiment'] = stats.apply(lambda x: x.total_sentiment / x.word_count, axis=1)\n",
    "\n",
    "\n",
    "current = stats[stats.term == CURRENT_TERM]\n",
    "\n",
    "# CURRENT_WEEK = current.week.max()\n",
    "historical = stats[stats.term != CURRENT_TERM]\n",
    "historical = historical[historical.week <= CURRENT_WEEK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In every class I teach, students are required to submit a weekly journal. The assignment is extremely low stakes (this semester, each journal is worth 0.33% of the total course grade).\n",
    "\n",
    "Journal assignments have no prescribed topic. The only requirement is that it be 'something related to class'. I tell them the point is to prompt them to reflect a bit on the class every week and to use the assignment in whatever way is useful for them. \n",
    "\n",
    "Most students use the journal to summarize recent materials. Some give me feedback on how the class is going. Many relate course topics to things in their lives and talk about their lives and how things are going generally.\n",
    "\n",
    "Most students turn in the journals almost every week; most students miss a couple of them. Students report missing journals because of other commitments (e.g., exams) and disruptions in their personal life (e.g., changes of work schedule, feeling overwhelmed by some crisis). Missing 2 consecutive journals is a reliable sign that I should reach out to the student. (Indeed, it's reliable enough that I've written a script to automatically send a message gently inquiring about what's going on after 2 consecutive missed journals). \n",
    "\n",
    "Thus it seems possible that trends in journal submission are somewhat sensitive (but not specific) measures of student engagement with my class and perhaps their overall well-being.\n",
    "\n",
    "## Population\n",
    "All data here is for my Philosophy 305 Business ethics course. Most of my students are juniors/seniors. Most have majors or premajors in COBAE, though there's a good smattering of majors across the colleges. Total enrollments: {'F18': 115, 'S19': 124, 'F19': 167, 'S20': 159}\n",
    "\n",
    "One major complication is course modality. The classes in F18 and S19 were all face-to-face. F19 was a mix of hybrid and face-to-face (approx 50-50). All classes in S20 were scheduled as fully online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots() #figsize=(9,3))\n",
    "sns.lineplot(x=\"week\", y=\"pct_completion\", data=historical, label='historical', ax=axes)\n",
    "g = sns.lineplot(x='week', y=\"pct_completion\", color='red', label='current', data=current, ax=axes)\n",
    "g.set_title(\"Students submitting / Enrollment\")\n",
    "g.axvline(x=9, label='S20 spring break', color='green', linestyle='--')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purple line is the mean completion percentage for all previous semesters. \n",
    "\n",
    "Shaded region is bootstrapped 95% CI from historical data. (I'm not sure why breaks down in weeks 5-7)\n",
    "\n",
    "Vertical dashed line is Spring 20's spring break. Note that historical data combines fall and spring semesters.\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "It looks clear that between the week 7 and 8 journals, the percentage of students completing journals fell substantially and has remained below historical average trend.\n",
    "\n",
    "\n",
    "### Notes\n",
    "With one exception, journals are due at the end of the named week. Thus the week 7 journal was due at 11.59 pm March 8. However, there is a 1 week grace period, so the last opportunity to turn in the week 7 journal was March 15. The vast majority of those who turn in a journal do so before the grace period. Unfortunately, the one exception is this semester's week 8 journal. I accidentally had it due (3/22) after spring break. That may explain the large drop --I need to look more closely. Though it doesn't explain the below trend rates for weeks 10 and 11.\n",
    "\n",
    "\n",
    "I've been moving the class away from its semi-synchronous design to a mostly asynchronous model. We made the final step in that direction last week. The journals are the one assignment which still must be done on a set schedule by everyone. I really, really hope the recent precipitous drop is due to students being confused by the changes.... \n",
    "\n",
    "Interestingly, the drop which historically occurs in week 5 consists almost entirely of students who otherwise do well on the exams and complete most higher stakes assignments. It's also not made up of the same students every week --there's not a big group that stops doing them completely, but there is a group which starts doing them more sporadically. Why week 5? My guess is that that's when the first accounting and business gateway midterms hit.\n",
    "\n",
    "Since this is the first semester I've taught fully-online and the structure of the course is very different, I'm unsure how to interpret the trends this semester, even before week 8 when it was becoming clear that the university would go online.\n",
    "\n",
    "### Tables (including sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = current.drop(['num_empty'], axis=1).set_index(['term', 'week'])\n",
    "display(Latex(df.to_latex(caption=\"Current semester\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = historical.drop(['num_empty'], axis=1).set_index(['term', 'week'])\n",
    "display(Latex(df.to_latex(caption=\"Past semesters\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this is\n",
    "To get an extremely rough sense of what students are feeling, I've repurposed some tools I use to analyze pain patient narratives and patient twitter use. \n",
    "\n",
    "\n",
    "## Method\n",
    "What I'm calling the 'sentiment score' is based on a wordlist approach that's validated on twitter \n",
    "data. \n",
    "\n",
    "For each week of each semester, I have combined all the text submitted by students in all classes. Thus all analysis is happening on a week in a term and lumps together multiple classes. I have removed the most common English words (stopwords) and common words like 'swenson', 'business', 'journals', et cetera.\n",
    "\n",
    "I then used the Afinn wordlist to assign each word a score between -3 and 3 based on whether it tends to be used in negatively or positively. These scores are totaled and divided by the word count to allow comparison between collections of different length. \n",
    "\n",
    "## Limitations\n",
    "Please do not read anything into the scores. They are NOT measures of student satisfaction with the class. They simply show the relative amounts of words which tend to be used positively and words which tend to be used negatively.\n",
    "\n",
    "I am extremely suspicious of using a tool validated on twitter data for other uses, though I am unaware of any clear evidence justifying that suspicion.\n",
    "\n",
    "Even if the sentiment scores are valid, there are a lot of confounding factors in interpreting them. For example, week 2-4 is probably so negative because that's when we are discuss prisoners' dilemmas and tragedies of the commons. The journals are full of words like 'tragedy', 'disaster', 'prison', 'cheat', 'deceit', et cetera.\n",
    "\n",
    "Thus the absolute scores are pretty useless. However, there could be some value in comparing scores across semesters for the same week. There are still lots of limitations here. Inter alia, the topics don't exactly align, dates of breaks and exams are different, and the structure of the present semester is completely different. For example, we will not be talking about harm and the harm principle this semester, though it was previously a main topic. \n",
    "\n",
    "With all those caveats firmly in mind...\n",
    "\n",
    "## Current semester vs. past mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots() #figsize=(9,3))\n",
    "sns.lineplot(x=\"week\", y=\"sentiment\", data=historical, label='historical', ax=axes)\n",
    "g = sns.lineplot(x='week', y=\"sentiment\", color='red', label='current', data=current, ax=axes)\n",
    "g.set_title(\"Total sentiment score / word count by week\")\n",
    "g.axvline(x=9, label='S20 spring break', color='green', linestyle='--')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Bearing in mind the extremely tenuous nature of this analysis, it looks like we have the reverse of what we saw above in the percentage of students completing journals. It appears that in week 7, the language used shifted to tend more positive than in the past. A similar jump occurs in week 12. \n",
    "\n",
    "The fact that the big drops in submissions in week 7 and week 12 accompany increases in the sentiment score is interesting. This suggests that students who are more prone to use negative-tending words stopped submitting journals. \n",
    "\n",
    "Insofar as the sentiment score reflects what students are feeling, it may be that the students who feel worse are disengaging with my class. That's extremely troubling.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "This needs to be taken with a truckload of salt. \n",
    "\n",
    "#### What is the sentiment score tracking?\n",
    "To reiterate, it is far from clear what the sentiment score tracks. \n",
    "\n",
    "Indeed, I'm reluctant to try formally testing any hypothesis here without a sentiment scoring tool that's validated on student journals. I've refrained from examining/presenting statistical measures for this reason. I have sporadically worked on a machine learning based tool for this in the past; but I don't expect it to ever really work.\n",
    "\n",
    "#### Misalignment of topics\n",
    "Again, it is possible that some of the difference between the present and past semesters is due to a misalignment of class topics. Indeed, the topics discussed around week 12 in past semesters include the concept of harm and harms related to information security / data privacy. I've removed those topics this semester because the discussion is often very personal and raw --students often volunteer difficult experiences including identity theft affecting family members, stalking, and undocumented students' concerns about state surveillance. I'm not confident in my ability to manage that discussion when I can't see the student in the corner tearing up and looking anxiously at the door. The topics will come back into alignment around week 13. Though, even then, our move to a more asynchronous model may raise the same problem.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semesters disagreggated\n",
    "\n",
    "(Not sure if there's anything interesting here or how best to visualize, hence the multiple plots.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(9, 3))\n",
    "order = ['F18', 'S19', 'F19', 'S20']\n",
    "sns.barplot(x=\"week\", y=\"sentiment\", hue=\"term\", data=stats, hue_order=order, ax=axes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(9, 3))\n",
    "sns.lineplot(x=\"week\", y=\"sentiment\", hue=\"term\", data=stats, hue_order=order, ax=axes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of sentiment scores\n",
    "\n",
    "Above, we were looking at sentiment scores applied to the entire corpus of text submitted by students. The following scores student's journal entry separately and displays the distributions of those scores. \n",
    "\n",
    "NB, looking at individual sentiment scores without a properly validated assessment tool may magnify the issues discussed above. But let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individ_sentiments = { w : [] for w in weeks } \n",
    "inds = []\n",
    "for d in data:\n",
    "    g = 'current' if d.term == CURRENT_TERM else 'historic' \n",
    "    inds.extend( [ {'week' : d.week_num, 'student_sent': s, 'g': g} for s in d.student_avg_sentiments])\n",
    "\n",
    "inds = pd.DataFrame(inds)\n",
    "inds = inds[inds.week <= CURRENT_WEEK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, figsize=(10,8))\n",
    "sns.violinplot(x=\"week\", y=\"student_sent\", hue=\"g\", split=True, palette='deep',  data=inds, ax=axes[0]);\n",
    "sns.boxplot(x=\"week\", y=\"student_sent\", hue='g', palette='deep', data=inds, ax=axes[1]);\n",
    "axes[0].set_ylim((-0.25, 0.25));axes[0].set_ylim((-0.4, 0.4))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams (Current semester only)\n",
    "\n",
    "What are students actually talking about..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from CanvasHacks.Text.ngrams import BigramGetter, TrigramGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgrams = {}\n",
    "\n",
    "tgrams = {}\n",
    "\n",
    "for ws in week_stores:\n",
    "    if ws.term == CURRENT_TERM:\n",
    "        bgrams[ws.week] = BigramGetter()\n",
    "        bgrams[ws.week].process(ws.no_stops_bag, min_freq=3, get_top=10)\n",
    "        tgrams[ws.week] = TrigramGetter()\n",
    "        tgrams[ws.week].process(ws.no_stops_bag, min_freq=3, get_top=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top PMI\n",
    "Returns top 10 with highest Pointwise Mutual Information (i.e., which occur together more often than would be expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    print(\"========== week {} ==========\".format(w))\n",
    "    for b in bgrams[w].topPMI:\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top likelihood ratio\n",
    "Likelihood ratio is similar to tf-idf. It relates the frequency of word1, frequency of word2, and frequency of word1 word2 in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    print(\"========== week {} ==========\".format(w))\n",
    "    for b in bgrams[w].top_likelihood_ratio:\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw frequencies (top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    print(\"========== week {} ==========\".format(w))\n",
    "    for b in bgrams[w].raw_freq[:20]:\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.Series([f for gram, f in bgrams[8].raw_freq])\n",
    "g = sns.distplot(d)\n",
    "g.set_title('distribution of frequencies (wk 8)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams\n",
    "### Top PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    print(\"========== week {} ==========\".format(w))\n",
    "    for t in tgrams[w].topPMI:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    print(\"========== week {} ==========\".format(w))\n",
    "    for t in tgrams[w].top_likelihood_ratio:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw frequencies (top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    print(\"========== week {} ==========\".format(w))\n",
    "    for t in tgrams[w].raw_freq[:20]:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual word freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from CanvasHacks.Text.stats import WordFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and clean\n",
    "\n",
    "SLOW: ~10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANGES_CUTOFF = 30\n",
    "\n",
    "freqs = []\n",
    "\n",
    "for d in week_stores:\n",
    "    fq = WordFreq(d.no_stops_bag)\n",
    "    for dct in fq.word_freq_dicts:\n",
    "        ddd = {\n",
    "            'term':  d.term,\n",
    "            'week' :  d.week_num,\n",
    "            'word': dct['word'],\n",
    "            'freq': dct['count']\n",
    "            }\n",
    "        freqs.append(ddd)\n",
    "freqs = pd.DataFrame(freqs)\n",
    "\n",
    "# Make data for term frequency\n",
    "past_sem = freqs[freqs.term != CURRENT_TERM]\n",
    "# unnecessary for the past sem since we get the counts in a different way\n",
    "cur_sem = freqs[freqs.term == CURRENT_TERM].drop(['term'], axis=1)\n",
    "\n",
    "avg_word_counts = []\n",
    "for g, v in past_sem.groupby(['week', 'word']):\n",
    "    avg_word_counts.append({\n",
    "        'week': g[0], \n",
    "        'word': g[1], \n",
    "        'avg_count' : v['freq'].mean()\n",
    "    })\n",
    "\n",
    "avg_word_counts = pd.DataFrame(avg_word_counts)\n",
    "# put in one frame\n",
    "a = avg_word_counts.set_index(['week', 'word'])\n",
    "b = cur_sem.set_index(['week', 'word'])\n",
    "comb = pd.concat([a, b], axis=1)\n",
    "comb.reset_index(inplace=True)\n",
    "comb.fillna(0, inplace=True)\n",
    "\n",
    "def calc_delta(row):\n",
    "    f = row.freq if not pd.isnull(row.freq) else 0\n",
    "    return f - row.avg_count\n",
    "\n",
    "comb = comb[~pd.isnull(comb.freq)]\n",
    "comb['cnt_delta'] = comb.apply(lambda x: calc_delta(x), axis=1)\n",
    "\n",
    "top_changes = []\n",
    "\n",
    "for w in weeks:\n",
    "    week_frame = comb[comb.week == w].copy(deep=True)\n",
    "    sorted_week = week_frame.sort_values('cnt_delta', ascending=False)\n",
    "    r = { 'week': w, \n",
    "         'top_increased':  sorted_week[ : CHANGES_CUTOFF],\n",
    "         'top_decreased': sorted_week[-CHANGES_CUTOFF : ]\n",
    "        }\n",
    "    top_changes.append(r)\n",
    "# top_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List changed frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Latex\n",
    "\n",
    "def display_increased_freq(frame, week):\n",
    "    f = [f for f in filter(lambda x: x['week'] == week, frame)][0]\n",
    "    t = \"Top increased freq (week {})\".format(week)\n",
    "    display(Latex(f['top_increased'].to_latex(caption=t)))\n",
    "\n",
    "def display_decreased_freq(frame, week):\n",
    "    f = [f for f in filter(lambda x: x['week'] == week, frame)][0]\n",
    "    t = \"Top decreased freq (week {})\".format(week)\n",
    "    display(Latex(f['top_decreased'].to_latex(caption=t)))\n",
    "\n",
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    if w != 9: #spring break\n",
    "        display_increased_freq(top_changes, w)\n",
    "        display_decreased_freq(top_changes, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordclouds (term use and change from past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from CanvasHacks.Text.VisualizationTools import draw_cloud, draw_cloud_from_freqs, draw_cumulative_freq, clearplot_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordclouds(frame, week):\n",
    "    \"\"\"Draws frequency wordclouds\"\"\"\n",
    "    d = frame[frame.week == week].copy(deep=True)\n",
    "\n",
    "    # make back into freqdist-like objects\n",
    "    past_dists = {}\n",
    "    current_dists = {}\n",
    "    increases = {}\n",
    "    decreases = {}\n",
    "\n",
    "    for i, row in d.iterrows():\n",
    "        past_dists[row.word] = row.avg_count\n",
    "        current_dists[row.word] = row.freq\n",
    "        if row.cnt_delta > 0:\n",
    "            increases[row.word] = row.cnt_delta\n",
    "        if row.cnt_delta < 0:\n",
    "            decreases[row.word] = row.cnt_delta * -1\n",
    "\n",
    "    draw_cloud_from_freqs(current_dists, title=\"Current semester frequencies (week {})\".format(week))\n",
    "#     draw_cloud_from_freqs(past_dists, title=\"Past semester frequencies (week {})\".format(week))\n",
    "    draw_cloud_from_freqs(increases, title=\"Increased frequency over past (week {})\".format(week))\n",
    "    draw_cloud_from_freqs(decreases, title=\"Decreased frequency over past (week {})\".format(week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(7, CURRENT_WEEK +1):\n",
    "    if w != 9: #spring break\n",
    "        draw_wordclouds(comb, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearplot_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = []\n",
    "for w, v in comb.groupby('week'):\n",
    "    z = v.cnt_delta.max()\n",
    "#     z = v.sort_values('cnt_delta', axis=1)[:5]\n",
    "    j.append((w, z))\n",
    "len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# All the stored bags may have been cleaned in different ways\n",
    "# when stored. Thus we clean again to make sure standard\n",
    "token_filter = TokenFiltrationMixin()\n",
    "\n",
    "def filter_on_regex(word, rx=token_filter.to_remove_inc_stops_regex):\n",
    "    if rx.match(word) is None:\n",
    "        return word\n",
    "\n",
    "freqs.word = freqs.apply(lambda x: filter_on_regex(x.word), axis=1)\n",
    "freqs.dropna(inplace=True)\n",
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "journal entries\n",
    "\n",
    "    term: S/F year (S18, F20)\n",
    "   \n",
    "    course_id: Canvas course id\n",
    "   \n",
    "    id': canvas id of the journal assignment \n",
    "    \n",
    "    week_num: Integer of the week of the joural \n",
    "    \n",
    "    content: List of journal entries\n",
    "\n",
    "journal entry\n",
    "\n",
    "     sid: Author's canvas id\n",
    "        \n",
    "     body: Text after removing html tags. NB., may be blank of student never turned in. Keeping this since can be proxy for engagement\n",
    "        \n",
    "     bag: Wordbag including stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated by week\n",
    "\n",
    "week_stats = []\n",
    "hg = historical.groupby('week')\n",
    "for week, h in hg:\n",
    "    s = {'week' : week,\n",
    "         'mean_word_count' : h.word_count.mean(),\n",
    "         'avg_word_sentiment' : h.avg_word_sentiment.mean()}\n",
    "    week_stats.append(s)\n",
    "week_stats = pd.DataFrame(week_stats)\n",
    "\n",
    "# week_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def make_datelist(words):\n",
    "    datelist = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            d = pd.to_datetime(w)\n",
    "            datelist.append(w)\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "    return datelist\n",
    "\n",
    "datelist = make_datelist(w.word.tolist())\n",
    "len(datelist)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj.set_index(['week', 'word'])\n",
    "jj.iloc[(1, \"'*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make for terms\n",
    "\n",
    "\n",
    "    term: S/F year (S18, F20)\n",
    "   \n",
    "    week_num\n",
    "    \n",
    "    bag: Concatenation of all bags for the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    print(d.term, d.week_num)\n",
    "    print(d.word_count)\n",
    "    assert(d.word_count is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentiment(bag):\n",
    "    if len(bag) == 0:\n",
    "        return None\n",
    "    txt = ' '.join(bag)\n",
    "    return afinn.score(txt)\n",
    "\n",
    "t =['do',\n",
    " 'it',\n",
    " 'that',\n",
    " 'bad']\n",
    "calc_sentiment(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stat(\n",
    "            term=term,\n",
    "            week=week_num,\n",
    "            total_sentiment=calc_sentiment(out['combo_bag']),\n",
    "            word_count=len(out['combo_bag']),\n",
    "            num_empty=num_empty,\n",
    "            num_students=num_students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combo_bag(term, week_num, course_journals):\n",
    "    \"\"\"\n",
    "    Combines all the wordbags for the week \n",
    "    and computes various stats along the way.\n",
    "    Returns a dictionary with a combined wordbag and stat objects\n",
    "    \n",
    "    Entries is a list of 0 or more objects\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    out['combo_bag'] = []\n",
    "    out['stat'] = None\n",
    "    \n",
    "    num_entries = 0\n",
    "    \n",
    "    if len(course_journals) > 0:\n",
    "        # todo Add a parallel creation of no stop stat objects once\n",
    "        # we have a stored no_stop_bag\n",
    "        num_empty = 0    \n",
    "        # Make the combo bag\n",
    "        for e in entries[0]['content']:\n",
    "            num_students = len(e['bag'])\n",
    "            if num_students == 0:\n",
    "                num_empty += 1\n",
    "            out['combo_bag'] += e['bag']\n",
    "        \n",
    "        # Create statistics\n",
    "        out['stat'] = Stat(\n",
    "            term=term,\n",
    "            week=week_num,\n",
    "            total_sentiment=calc_sentiment(out['combo_bag']),\n",
    "            word_count=len(out['combo_bag']),\n",
    "            num_empty=num_empty,\n",
    "            num_students=num_students)\n",
    "\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data = []\n",
    "\n",
    "for t in terms:\n",
    "    for w in weeks:\n",
    "        try:\n",
    "            entries = [ d for d in data if d['term'] == t and d['week_num'] == w]\n",
    "            print(len(entries))\n",
    "            try:\n",
    "                r = make_combo_bag(t, w, entries)\n",
    "                store[t][w] = r['combo_bag']\n",
    "                stats_data.append(r['stat'])\n",
    "            \n",
    "            except IndexError:\n",
    "                print('index error ', t, w)\n",
    "\n",
    "            \n",
    "\n",
    "#             combo_bag = []\n",
    "#             if len(entries) > 0:\n",
    "#                 # todo Add a parallel creation of no stop stat objects once\n",
    "#                 # we have a stored no_stop_bag \n",
    "#                 num_empty = 0\n",
    "#                 for e in entries[0]['content']:\n",
    "#                     num_entries = len(e['bag'])\n",
    "#                     if num_entries == 0:\n",
    "#                         num_empty += 1\n",
    "#                     combo_bag += e['bag']\n",
    "                    \n",
    "\n",
    "#                 store[t][w] = combo_bag\n",
    "#                 s = Stat(term=t, \n",
    "#                          week=w, \n",
    "#                          total_sentiment=calc_sentiment(combo_bag), \n",
    "#                          word_count=len(combo_bag),\n",
    "#                          num_empty=num_empty, \n",
    "#                         num_entries=num_entries)\n",
    "#                 stats_data.append(s)\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stat\n",
    "\n",
    "    term\n",
    "    \n",
    "    week\n",
    "    \n",
    "    total_sentiment\n",
    "    \n",
    "    word_count\n",
    "    \n",
    "    num_empty\n",
    "    \n",
    "    num_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combo_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = [ d for d in data if d['term'] == 'F18' and d['week_num'] == 10]\n",
    "\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare number of non submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course_id, j in journals.items():\n",
    "    assignment = environment.CONFIG.course.get_assignment(int(assignments[0][0]))\n",
    "# Download submissions\n",
    "subRepo = SubmissionRepository(assignment)\n",
    "\n",
    "bodies = [d.body for d in subRepo.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = [bagmaker.process(TextCleaner.clean(b)) for b in bodies if b is not None]\n",
    "len(bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [afinn.score(b) for b in bodies if b is not None]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagmaker.process('I love dogs! Dogs are the best!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn.score('I love dogs! Dogs are the best!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn.score('I hate you Die! Die! Die! Die!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for a in environment.CONFIG.assignments:\n",
    "    # canvas api object\n",
    "    assignment = course.get_assignment(int(a[0]))\n",
    "    # activity object to define the features \n",
    "    journal = Journal(**assignment.attributes)\n",
    "    # Download submissions\n",
    "    subRepo = SubmissionRepository(assignment)\n",
    "    if GRADING_LATE:\n",
    "        # parse out already graded submissions\n",
    "        subRepo.data =[j for j in subRepo.data if j.grade != 'complete']\n",
    "\n",
    "    # shove the activity onto a sub repo so it will resemble\n",
    "    # a quizrepo for the grader\n",
    "    subRepo.activity = journal\n",
    "    # Initialize the package for results\n",
    "    store = DataStoreNew(journal)\n",
    "    # provisionally determine credit\n",
    "    grader = JournalGrader(subRepo)\n",
    "    store.results = grader.grade()\n",
    "\n",
    "    results.append(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment\n",
    "\n",
    "ideas\n",
    "\n",
    "calculate the sentiment score for each student's bag and display overlapping kdes for each week\n",
    "\n",
    "calclulate a global class sentimenet score for each week\n",
    "\n",
    "weight each word sentiment score by its tf-idf in the journal entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average_sentiment(bag):\n",
    "    return afinn.score(bag) / len(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canv-env",
   "language": "python",
   "name": "canv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "511.1px",
    "width": "251.1px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
